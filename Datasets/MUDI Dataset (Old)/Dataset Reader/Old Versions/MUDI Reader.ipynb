{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Initial* **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Library** *Settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Download\n",
    "%pip install --upgrade pip\n",
    "%pip install numpy\n",
    "%pip install argparse\n",
    "%pip install pandas\n",
    "%pip install torch\n",
    "%pip install matplot\n",
    "%pip install plotly\n",
    "%pip install itk\n",
    "%pip install itkwidgets\n",
    "%pip install h5py\n",
    "%pip install h5pyViewer\n",
    "%pip install nilearn\n",
    "%pip install dipy\n",
    "%pip install openpyxl\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import itk\n",
    "import itkwidgets\n",
    "import h5py\n",
    "import nilearn\n",
    "import dipy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionality Import\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Callable, Dict, Literal, Optional, Union, Tuple\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn.image import smooth_img, load_img\n",
    "from nilearn.masking import apply_mask, unmask\n",
    "#from dipy.reconst.shm import cart2sphere, real_sh_descoteaux_from_index, sph_harm_ind_list\n",
    "from ipywidgets import interactive, IntSlider\n",
    "#from PIL import Image\n",
    "from tabulate import tabulate\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Control** *Station*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser Initialization\n",
    "parser = argparse.ArgumentParser(\n",
    "    description = \"MUDIVisualizer\")\n",
    "\n",
    "# Filepath Arguments\n",
    "path = parser.add_argument_group('Required Filepaths')\n",
    "path.add_argument('--param_filepath', type = Path, default = '../Raw Data/parameters_new.xlsx',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Parameters')\n",
    "path.add_argument('--mask_folderpath', type = Path, default = '../Patient Mask',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Parameters')\n",
    "path.add_argument('--patient_folderpath', type = Path, default = '../Patient Data',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Patient Information')\n",
    "path.add_argument('--info_filepath', type = Path, default = '../Raw Data/header1_.csv',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Parameters')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "parse = parser.parse_args(\"\")\n",
    "parse.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data* **Access**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Raw* -> *Patient* **Data Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Initialization Example\n",
    "data = hMUDI(parse.data_filepath,        # Dataset Initialization\n",
    "            parse.param_filepath,       # using Filepaths indicated\n",
    "            parse.patient_filepath,     # in the Parser Arguments\n",
    "            parse.y_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient Loop\n",
    "for p in range(data.patientInfo.shape[0] - 1):\n",
    "\n",
    "    # Patient Data Access from \n",
    "    pN = data.patientInfo['Patient'].iloc[p]\n",
    "    print(f\"Accessing Data for Patient {pN}...\")\n",
    "    pData = data.data[:].iloc[np.where(data.y == pN)[0]].T\n",
    "    assert(pData.shape[1] == data.patientInfo['Voxels'].iloc[p], f\"Patient Data Dimensions not Correct!\")\n",
    "\n",
    "    # Patient Data File Saving\n",
    "    pData.to_csv(f\"Patient Data/p{pN}.csv\")\n",
    "    del pN, pData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Horizontal** / **Voxel** *Reader*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal MUDI Dataset Initialization Class\n",
    "class hMUDI(Dataset):\n",
    "\n",
    "    # Constructor Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_filepath: Path,     # Path for File containing Dataset Voxel Values\n",
    "        param_filepath: Path,       # Path for File containing all 1344 Parameter Settings Combination\n",
    "        patient_filepath: Path,     # Path for File Containing Number of Voxels per Patients\n",
    "        y_filepath: Path,           # Path for File Containing Voxels per Patient\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(hMUDI).__init__()\n",
    "        self.dataset_filepath = dataset_filepath\n",
    "        self.param_filepath = param_filepath\n",
    "        self.patient_filepath = patient_filepath\n",
    "        self.y_filepath = y_filepath\n",
    "\n",
    "        # Required Memory Space\n",
    "        file_size = os.path.getsize(dataset_filepath)\n",
    "        available_memory = psutil.virtual_memory().available\n",
    "        assert(available_memory >= file_size\n",
    "        ), f\"ERROR: Dataset requires {file_size}b, but only {available_memory}b is available!\"\n",
    "\n",
    "        # Dataset's Parameter & Patient Info Access\n",
    "        self.params = pd.read_excel(self.param_filepath)\n",
    "        self.patientInfo = pd.read_csv(self.patient_filepath)\n",
    "        self.patientInfo['Patient'].iloc[-1] = 'Total'\n",
    "        self.y = pd.read_csv(self.y_filepath)\n",
    "        self.y = pd.DataFrame(self.y['1'].values, columns = ['y'])\n",
    "        self.num_patients = self.patientInfo.shape[0] - 1       # Total Number of Patients in Dataset\n",
    "        self.num_params = self.params.shape[0]                  # Total Number of Parameters in Dataset\n",
    "\n",
    "        # Dataset's Value Access\n",
    "        if 'data' not in dir(self):\n",
    "            print(\"Now Downloading Data...\")\n",
    "            with h5py.File(dataset_filepath, 'r') as dataset:\n",
    "                self.data = pd.DataFrame(dataset.get('data1'))\n",
    "                #self.data.columns = (self.params.T).values.tolist()\n",
    "        \n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "    # Dataset Splitting Function\n",
    "    def split(\n",
    "        self,\n",
    "        percentage: bool = False,       # Control Variable for the Format of Parameter Limits\n",
    "        train_patients: int = 4,        # No. Patients used in Training\n",
    "        train_params: int = 500,        # No. / Percentage of Parameters used in Training\n",
    "        val_patients: int = 1,          # No. Patients used in Validation\n",
    "        val_params: int = 10,           # No. / Percentage of Parameters used in Validation\n",
    "        batch_size: int = 500,          # Sample Batch Size Value\n",
    "    ):\n",
    "\n",
    "        # General Assertions for Input Values\n",
    "        print(\"Proceeding to Data Splitting...\\n\")\n",
    "        assert( 0 < batch_size <= self.num_params                       # Limits for Batch Size \n",
    "        ), f\"ERROR: Requested Batch Size not Supported!\"\n",
    "        assert( 0 < train_patients <= self.num_patients                 # Limits for No. of Training Patients\n",
    "        ), f\"ERROR: Training Patient Number not Supported!\"\n",
    "        assert( 0 < val_patients <= self.num_patients                   # Limits for No. of Validation Patients\n",
    "        ), f\"ERROR: Validation Patient Number not Supported!\"\n",
    "        assert( train_patients + val_patients <= self.num_patients      # Limits for No. of Selected Patients\n",
    "        ), f\"ERROR: Total Patient Number not Supported!\"\n",
    "        assert( 0 < train_params <= self.num_params                     # Limits for No. of Training Parameters\n",
    "        ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "        assert( 0 < val_params <= self.num_params                       # Limits for No. of Validation Parameters\n",
    "        ), f\"ERROR: Validation Parameter Number not Supported!\"\n",
    "\n",
    "        # Split Datasets' Vertical Shape Initialization\n",
    "        self.train_patients = train_patients\n",
    "        self.train_samples = sum(self.patientInfo['Voxels'].iloc[0 : train_patients])\n",
    "        self.val_patients = val_patients\n",
    "        self.val_samples = sum(self.patientInfo['Voxels'].iloc[train_patients : train_patients + val_patients])\n",
    "        self.test_patients = self.num_patients - (train_patients + val_patients)\n",
    "        self.test_samples = sum(self.patientInfo['Voxels'].iloc[train_patients + val_patients : -1])\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Split Datasets' Vertical Shape Initialization (Percentage Value)\n",
    "        if(percentage):\n",
    "            \n",
    "            # Assertions for Parameter Input Values\n",
    "            assert( 0 < train_params <= 100                 # Percentage Limits for Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            assert(0 < val_params <= 100                    # Percentage Limits for Validation Parameters\n",
    "            ), f\"ERROR: Validation Parameter Number not Supported!\"\n",
    "            \n",
    "            # Class Variable Logging\n",
    "            self.trainTrain_params = train_params           # Percentage of Training Set's Training Parameters\n",
    "            self.trainVal_params = 100 - train_params       # Percentage of Training Set's Validation Parameters\n",
    "            self.valTrain_params = val_params               # Percentage of Validation Set's Training Parameters\n",
    "            self.valVal_params = 100 - val_params           # Percentage of Validation Set's Validation Parameters\n",
    "            if( self.test_patients != 0):\n",
    "                self.testTrain_params = val_params                      # Percentage of Test Set's Training Parameters\n",
    "                self.testVal_params = 100 - self.testTrain_params       # Percentage of Test Set's Validation Parameters\n",
    "        \n",
    "        # Split Datasets' Vertical Shape Initialization (Percentage Value)\n",
    "        else:\n",
    "\n",
    "            # Assertions for Parameter Input Values\n",
    "            assert( 0 < train_params <= self.num_params                 # Percentage Limits for Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            assert(0 < val_params <= self.num_params                    # Percentage Limits for Validation Parameters\n",
    "            ), f\"ERROR: Validation Parameter Number not Supported!\"\n",
    "            \n",
    "            # Class Variable Logging\n",
    "            self.trainTrain_params = train_params                                   # Percentage of Training Set's Training Parameters\n",
    "            self.trainVal_params = self.num_params - self.trainTrain_params         # Percentage of Training Set's Validation Parameters\n",
    "            self.valTrain_params = val_params                                       # Percentage of Validation Set's Training Parameters\n",
    "            self.valVal_params = self.num_params - self.valTrain_params             # Percentage of Validation Set's Validation Parameters\n",
    "            if( self.test_patients != 0):\n",
    "                self.testTrain_params = val_params                                  # Percentage of Test Set's Training Parameters\n",
    "                self.testVal_params = self.num_params - self.testTrain_params       # Percentage of Test Set's Validation Parameter\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Dataset Vertical / Sample Splitting (Without Test Set)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.data, self.y,\n",
    "            test_size = self.val_samples + self.test_samples,\n",
    "            shuffle = False, random_state = 42)\n",
    "\n",
    "        # Dataset Vertical / Sample Splitting (With Test Set)\n",
    "        if(self.test_patients != 0):\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_val, y_val,\n",
    "                test_size = self.test_samples,\n",
    "                shuffle = False, random_state = 42)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Dataset Horizontal / Parameter Splitting | Training Set\n",
    "        X_trainTrain, X_trainVal, self.params_trainTrain, self.params_trainVal = train_test_split(\n",
    "                    X_train.T, self.params,\n",
    "                    test_size = self.trainVal_params,\n",
    "                    shuffle = parse.hShuffle, random_state = 42)\n",
    "        self.X_trainTrain = pd.concat([X_trainTrain.T, y_train], axis = 1)\n",
    "        self.X_trainVal = pd.concat([X_trainVal.T, y_train], axis = 1)\n",
    "        \n",
    "        # Dataset Horizontal / Parameter Splitting | Validation Set\n",
    "        X_valTrain, X_valVal, self.params_valTrain, self.params_valVal = train_test_split(\n",
    "                    X_val.T, self.params,\n",
    "                    test_size = self.valVal_params,\n",
    "                    shuffle = parse.hShuffle, random_state = 42)\n",
    "        self.X_valTrain = pd.concat([X_valTrain.T, y_val], axis = 1)\n",
    "        self.X_valVal = pd.concat([X_valVal.T, y_val], axis = 1)\n",
    "\n",
    "        # Dataset Horizontal / Parameter Splitting | Testing Set\n",
    "        if( self.test_patients != 0):\n",
    "            X_testTrain, X_testVal, self.params_testTrain, self.params_testVal = train_test_split(\n",
    "                        X_test.T, self.params,\n",
    "                        test_size = self.testVal_params,\n",
    "                        shuffle = parse.hShuffle, random_state = 42)\n",
    "            self.X_testTrain = pd.concat([X_testTrain.T, y_test], axis = 1)\n",
    "            self.X_testVal = pd.concat([X_testVal.T, y_test], axis = 1)\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # DataLoader Building\n",
    "        self.dl_trainTrain = DataLoader(torch.tensor(self.X_trainTrain.values, dtype = torch.float32),\n",
    "                                        batch_size = self.batch_size, shuffle = parse.vShuffle)\n",
    "        self.dl_trainVal = DataLoader(torch.tensor(self.X_trainVal.values, dtype = torch.float32),\n",
    "                                        batch_size = self.batch_size, shuffle = parse.vShuffle)\n",
    "        self.dl_valTrain = DataLoader(torch.tensor(self.X_valTrain.values, dtype = torch.float32),\n",
    "                                        batch_size = self.batch_size, shuffle = parse.vShuffle)\n",
    "        self.dl_valVal = DataLoader(torch.tensor(self.X_valVal.values, dtype = torch.float32),\n",
    "                                        batch_size = self.batch_size, shuffle = parse.vShuffle)\n",
    "        if (self.test_patients != 0):\n",
    "            self.dl_testTrain = DataLoader(torch.tensor(self.X_testTrain.values, dtype = torch.float32),\n",
    "                                        batch_size = self.batch_size, shuffle = parse.vShuffle)\n",
    "            self.dl_testVal = DataLoader(torch.tensor(self.X_testVal.values, dtype = torch.float32),\n",
    "                                        batch_size = self.batch_size, shuffle = parse.vShuffle)\n",
    "                                        \n",
    "        #Split Datasets' Contents Report\n",
    "        if(percentage):\n",
    "            print(tabulate([[self.train_patients, self.train_samples, f\"{(self.trainTrain_params / 100) * self.num_params} ({np.round(self.trainTrain_params, 2)}%)\", f\"{(self.trainVal_params / 100) * self.num_params} ({np.round(self.trainVal_params, 2)}%)\"],\n",
    "                            [self.val_patients, self.val_samples, f\"{(self.valTrain_params / 100) * self.num_params} ({np.round(self.valTrain_params, 2)}%)\", f\"{(self.valVal_params / 100) * self.num_params} ({np.round(self.valVal_params, 2)}%)\"],\n",
    "                            [self.test_patients, self.test_samples, f\"{(self.valTrain_params / 100) * self.num_params} ({np.round(self.valTrain_params, 2)}%)\", f\"{(self.valVal_params / 100) * self.num_params} ({np.round(self.valVal_params, 2)}%)\"],\n",
    "                            [self.num_patients, (self.train_samples + self.val_samples + self.test_samples), \"\", \"\"]],\n",
    "                            headers = ['No. Patients', 'No. Voxels', 'Training Parameters', 'Validation Parameters'],\n",
    "                            showindex = ['Training Set', 'Validation Set', 'Test Set', 'Total'], tablefmt = 'fancy_grid'))\n",
    "        else:\n",
    "            print(tabulate([[self.train_patients, self.train_samples, f\"{self.trainTrain_params} ({np.round((self.trainTrain_params / self.num_params) * 100, 2)}%)\", f\"{self.trainVal_params} ({np.round((self.trainVal_params   / self.num_params) * 100, 2)}%)\"],\n",
    "                            [self.val_patients, self.val_samples, f\"{self.valTrain_params} ({np.round((self.valTrain_params / self.num_params) * 100, 2)}%)\", f\"{self.valVal_params} ({np.round((self.valVal_params   / self.num_params) * 100, 2)}%)\"],\n",
    "                            [self.test_patients, self.test_samples, f\"{self.valTrain_params} ({np.round((self.valTrain_params / self.num_params) * 100, 2)}%)\", f\"{self.valVal_params} ({np.round((self.valVal_params   / self.num_params) * 100, 2)}%)\"],\n",
    "                            [self.num_patients, (self.train_samples + self.val_samples + self.test_samples), \"\", \"\"]],\n",
    "                            headers = ['No. Patients', 'No. Voxels', 'Training Parameters', 'Validation Parameters'],\n",
    "                            showindex = ['Training Set', 'Validation Set', 'Test Set', 'Total'], tablefmt = 'fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Initialization Example\n",
    "data = hMUDI(parse.data_filepath,        # Dataset Initialization\n",
    "            parse.param_filepath,       # using Filepaths indicated\n",
    "            parse.patient_filepath,     # in the Parser Arguments\n",
    "            parse.y_filepath)\n",
    "data.split()                            # Dataset Splitting using Default Values\n",
    "\n",
    "# DataLoader Usage Example (Training Parameters for Training Set)\n",
    "assert(len(data.dl_trainTrain) == int(np.ceil(data.train_samples / data.batch_size)))\n",
    "for i, batch in enumerate(data.dl_trainTrain):\n",
    "    print(f\"Batch #{i + 1}: {batch.shape[0]} Voxels / Samples | {batch.shape[1]} Settings / Parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1D Vertical** / **Image** *Reader*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical 1D MUDI Dataset Initialization Class\n",
    "class v1DMUDI(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_folderpath: Path,       # Path for Folder Containing Patient Data Files\n",
    "        param_filepath: Path,           # Path for File containing all 1344 Parameter Settings Combination\n",
    "        info_filepath: Path,            # Path for List of Patients and Corresponding Image Size inside Full Dataset\n",
    "    ):\n",
    "\n",
    "        # Parameter Value Access\n",
    "        super(v1DMUDI).__init__()\n",
    "        self.params = pd.read_excel(param_filepath)             # List of Dataset's Parameters\n",
    "        self.num_params = self.params.shape[0]                  # Total Number of Parameters in Dataset\n",
    "\n",
    "        # Patient Information Access\n",
    "        self.patient_folderpath = patient_folderpath\n",
    "        self.patient_info = pd.read_csv(info_filepath)          # List of Patients and Corresponding IDs & Image Sizes inside Full Dataset\n",
    "        self.patient_info = self.patient_info[:-1]              # Eliminating the Last Row containing Useless Information from the Patient Information\n",
    "        self.num_patients = self.patient_info.shape[0]          # Number of Patients inside Full Dataset\n",
    "\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # 1D Image Pre-Processing Function\n",
    "    def pre_process(\n",
    "        self,\n",
    "        img: pd.DataFrame,\n",
    "    ):\n",
    "\n",
    "        # Input Variable Assertions\n",
    "        assert(img.ndim == 2), \"ERROR: Input Image Shape not Supported! (2D Arrays only)\"\n",
    "        assert(self.pre_shape < img.shape[1]), \"ERROR: Convolution Layer Size must be smaller than Original Image's no. of Voxels!\"\n",
    "\n",
    "        # Dimensionality Reduction\n",
    "        img = np.array(img.values)\n",
    "        img_final = self.pca.fit_transform(img)\n",
    "        return pd.DataFrame(img_final)\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Patient Data Access & Splitting Function\n",
    "    def split_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "        train_params: int = 500,            # Number / Percentage of Parameters to be used in the Training Section of the Patient\n",
    "        percentage: bool = False,           # Control Variable for the Usage of Percentage Values in train_params\n",
    "        sample_shuffle: bool = False,       # Ability to Shuffle the Samples inside both Training and Validation Datasets\n",
    "    ):\n",
    "        \n",
    "        # Patient Data Access (including all Requirements)\n",
    "        assert(0 <= patient_number < self.num_patients), f\"ERROR: Input Patient not Found!\"         # Assertion for the Existence of the Requested Patient\n",
    "        patient_id = self.patient_info['Patient'].iloc[patient_number]                              # Patient ID contained within the Patient List\n",
    "        patient_filepath = Path(f\"{self.patient_folderpath}/p{patient_id}.csv\")                     # Patient Filepath from detailed Folder\n",
    "        assert(patient_filepath.exists()                                                            # Assertion for the Existence of Patient File in said Folder\n",
    "        ), f\"Filepath for Patient {patient_id} is not in the Dataset!\"\n",
    "        file_size = os.path.getsize(patient_filepath)                                               # Memory Space occupied by Patient File\n",
    "        available_memory = psutil.virtual_memory().available                                        # Memory Space Available for Computation\n",
    "        assert(available_memory >= file_size                                                        # Assertion for the Existence of Available Memory Space\n",
    "        ), f\"ERROR: Dataset requires {file_size}b, but only {available_memory}b is available!\"\n",
    "        pX = pd.read_csv(patient_filepath); del pX['Unnamed: 0']                                    # Full Patient Data\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(percentage):\n",
    "            assert(0 < train_params <= 100                              # Percentage Limits for Number of Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            train_params = train_params / 100                           # Percentage Value for Training Parameters\n",
    "            val_params = 1 - train_params                               # Percentage Value for Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Numerical Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params                  # Numerical Limits for Number of Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            val_params = self.num_params - train_params                 # Numerical Value for Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Patient Dataset Splitting into Training & Validation Sets\n",
    "        py = self.params; py['Patient'] = patient_id                # Patient Data Label Handling\n",
    "        pX = self.pre_process(pX)                                   # Patient Data Preprocessing\n",
    "        pX_train, pX_val, py_train, py_val = train_test_split(  pX, py,\n",
    "                                                                test_size = val_params,\n",
    "                                                                shuffle = sample_shuffle,\n",
    "                                                                random_state = 42)\n",
    "        return pX_train, pX_val, py_train, py_val\n",
    "        \n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Dataset Splitting Function\n",
    "    def split(\n",
    "        self,\n",
    "        test_patients: int = 1,             # Number of Patients to be used in the Test Set\n",
    "        train_params: int = 500,            # Number / Percentage of Parameters for the Training of the Training Set\n",
    "        test_params: int = 20,              # Number / Percentage of Parameters for the Training of the Test Set\n",
    "        pre_shape: int = 1200,              # Intermediate Dataset Shape as of Pre-Processing\n",
    "        percentage: bool = False,           # Control Variable for the Usage of Percentage Values in train_params\n",
    "        patient_shuffle: bool = False,      # Ability to Shuffle the Patients that compose both Training / Validation and Test Datasets\n",
    "        sample_shuffle: bool = False,       # Ability to Shuffle the Samples inside both Training / Validation and Test Datasets\n",
    "    ):\n",
    "\n",
    "        # Patient Number Variable Logging\n",
    "        assert(0 < test_patients <= self.num_patients               # Limits for Number of Test Set Patients\n",
    "        ), f\"ERROR: Test Patient Number not Supported!\"\n",
    "        self.train_patients = self.num_patients - test_patients     # Number of Patients to be used in the Training & Validation Sets\n",
    "        self.test_patients = test_patients                          # Number of Patients to be used in the Test Sets\n",
    "        self.pre_shape = pre_shape\n",
    "\n",
    "        # Pre-Processing Dimensionality Reduction + Patient Shuffling Feature\n",
    "        self.pca = PCA(n_components = self.pre_shape)\n",
    "        if(patient_shuffle):\n",
    "            self.patient_info = self.patient_info.iloc[np.random.permutation(len(self.patient_info))]\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(percentage):\n",
    "            assert(0 < train_params <= 100                              # Percentage Limits for Number of Training Set's Parameters\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"\n",
    "            self.trainTrain_params = train_params                       # Percentage Value for Training Set's Training Parameters\n",
    "            self.trainVal_params = 100 - train_params                   # Percentage Value for Training Set's Validation Parameters\n",
    "            assert(0 < test_params <= 100                               # Percentage Limits for Number of Test Set's Parameters\n",
    "            ), f\"ERROR: Test Set's Parameter Number not Supported!\"\n",
    "            self.testTrain_params = test_params                         # Percentage Value for Test Set's Training Parameters\n",
    "            self.testVal_params = 100 - test_params                     # Percentage Value for Test Set's Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params                  # Numerical Limits for Number of Training Set's Parameters\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"\n",
    "            self.trainTrain_params = train_params                       # Numerical Value for Training Set's Training Parameters\n",
    "            self.trainVal_params = self.num_params - train_params       # Numerical Value for Training Set's Validation Parameters\n",
    "            assert(0 < test_params <= self.num_params                   # Numerical Limits for Number of Test Set's Parameters\n",
    "            ), f\"ERROR: Test Set's Parameter Number not Supported!\"\n",
    "            self.testTrain_params = test_params                         # Numerical Value for Test Set's Training Parameters\n",
    "            self.testVal_params = self.num_params - test_params         # Numerical Value for Test Set's Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Training & Validation Sets Building\n",
    "        self.train_set = dict.fromkeys(('X_train', 'X_val', 'y_train', 'y_val'))\n",
    "        X_train = np.empty([0, self.pre_shape]); X_val = np.empty([0, self.pre_shape])\n",
    "        y_train = np.empty([0, 7]); y_val = np.empty([0, 7])\n",
    "        for p in range(self.train_patients):\n",
    "\n",
    "            # Training Patient Data Access & Treatment\n",
    "            print(f\"Adding Patient {self.patient_info['Patient'].iloc[p]}'s Data to the Training Set...\")       # Display of the Patient being Added to the Test Set\n",
    "            pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                    train_params = self.trainTrain_params,\n",
    "                                                                    percentage = percentage,\n",
    "                                                                    sample_shuffle = sample_shuffle)\n",
    "            X_train = np.concatenate((X_train, pX_train), axis = 0); X_val = np.concatenate((X_val, pX_val), axis = 0)\n",
    "            y_train = np.concatenate((y_train, py_train), axis = 0); y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "        \n",
    "        self.train_set['X_train'] = pd.DataFrame(X_train); self.train_set['X_val'] = pd.DataFrame(X_val)\n",
    "        self.train_set['y_train'] = pd.DataFrame(y_train); self.train_set['y_val'] = pd.DataFrame(y_val)\n",
    "        del X_train, X_val, y_train, y_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Test Set Building\n",
    "        self.test_set = dict.fromkeys(('X_train', 'X_val', 'y_train', 'y_val'))                             # Creation of Empty Dictionary to Fit Patient Data\n",
    "        X_train = np.empty([0, self.pre_shape]); X_val = np.empty([0, self.pre_shape])\n",
    "        y_train = np.empty([0, 7]); y_val = np.empty([0, 7])\n",
    "        for p in range(self.train_patients, self.train_patients + self.test_patients):\n",
    "\n",
    "            # Training Patient Data Access & Treatment\n",
    "            print(f\"Adding Patient {self.patient_info['Patient'].iloc[p]}'s Data to the Test Set...\")       # Display of the Patient being Added to the Test Set\n",
    "            pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                    train_params = self.testTrain_params,\n",
    "                                                                    percentage = percentage,\n",
    "                                                                    sample_shuffle = sample_shuffle)\n",
    "            X_train = np.concatenate((X_train, pX_train), axis = 0); X_val = np.concatenate((X_val, pX_val), axis = 0)\n",
    "            y_train = np.concatenate((y_train, py_train), axis = 0); y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "        \n",
    "        self.test_set['X_train'] = pd.DataFrame(X_train); self.test_set['X_val'] = pd.DataFrame(X_val)\n",
    "        self.test_set['y_train'] = pd.DataFrame(y_train); self.test_set['y_val'] = pd.DataFrame(y_val)\n",
    "        del X_train, X_val, y_train, y_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Split Datasets' Content Report\n",
    "        if(percentage):\n",
    "            print(tabulate([[self.train_patients, f\"{(self.trainTrain_params / 100) * self.num_params} ({self.trainTrain_params}%)\", f\"{(self.trainVal_params / 100) * self.num_params} ({self.trainVal_params}%)\"],\n",
    "                            [self.test_patients, f\"{(self.testTrain_params / 100) * self.num_params} ({self.testTrain_params}%)\", f\"{(self.testVal_params / 100) * self.num_params} ({self.testVal_params}%)\"]],\n",
    "                            headers = ['No. Patients', 'Training Parameters', 'Validation Parameters'],\n",
    "                            showindex = ['Training Set', 'Test Set'], tablefmt = 'fancy_grid'))\n",
    "        else:\n",
    "            print(tabulate([[self.train_patients, f\"{self.trainTrain_params} ({np.round((self.trainTrain_params / self.num_params) * 100, 2)}%)\", f\"{self.trainVal_params} ({np.round((self.trainVal_params / self.num_params) * 100, 2)}%)\"],\n",
    "                            [self.test_patients, f\"{self.testTrain_params} ({np.round(self.testTrain_params / self.num_params, 2)}%)\", f\"{self.testVal_params} ({np.round(self.testVal_params / self.num_params, 2)}%)\"]],\n",
    "                            headers = ['No. Patients', 'Training Parameters', 'Validation Parameters'],\n",
    "                            showindex = ['Training Set', 'Test Set'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "    \n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Dataset Saving Function\n",
    "    def save(\n",
    "        self,\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Vertical 1D MUDI (Version {version})', 'wb')\n",
    "        pickle.dump(self, f)\n",
    "        f.close\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loading Function\n",
    "    def load(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Vertical 1D MUDI (Version {version})', 'rb')\n",
    "        mudi = pickle.load(f)\n",
    "        f.close\n",
    "        return mudi\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Initialization Example\n",
    "mudi = v1DMUDI( parse.patient_folderpath,\n",
    "                parse.param_filepath,\n",
    "                parse.info_filepath)\n",
    "mudi.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "mudi = v1DMUDI.load(Path(f\"{main_folderpath}Saved Data\"), version = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3D Vertical** / **Image** *Reader*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical 3D MUDI Dataset Initialization Class\n",
    "class v3DMUDI(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_folderpath: Path,                   # Path for Folder Containing Patient Data Files\n",
    "        mask_folderpath: Path,                      # Path for Folder Containing Mask Data Files\n",
    "        param_filepath: Path,                       # Path for File containing all 1344 Parameter Settings Combination\n",
    "        info_filepath: Path,                        # Path for List of Patients and Corresponding Image Size inside Full Dataset\n",
    "    ):\n",
    "\n",
    "        # Parameter Value Access\n",
    "        super(v3DMUDI).__init__()\n",
    "        self.params = pd.read_excel(param_filepath)             # List of Dataset's Parameters\n",
    "        self.num_params = self.params.shape[0]                  # Total Number of Parameters in Dataset\n",
    "\n",
    "        # Patient Information Access\n",
    "        self.patient_folderpath = patient_folderpath\n",
    "        self.mask_folderpath = mask_folderpath\n",
    "        self.patient_info = pd.read_csv(info_filepath)          # List of Patients and Corresponding IDs & Image Sizes inside Full Dataset\n",
    "        self.patient_info = self.patient_info[:-1]              # Eliminating the Last Row containing Useless Information from the Patient Information\n",
    "        self.num_patients = self.patient_info.shape[0]          # Number of Patients inside Full Dataset\n",
    "\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    # Patient Data Access Function\n",
    "    def get_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "    ):\n",
    "\n",
    "        # Patient Data Access (including all Requirements)\n",
    "        assert(0 <= patient_number < self.num_patients), f\"ERROR: Input Patient not Found!\"         # Assertion for the Existence of the Requested Patient\n",
    "        patient_id = self.patient_info['Patient'].iloc[patient_number]                              # Patient ID contained within the Patient List\n",
    "        patient_filepath = Path(f\"{self.patient_folderpath}/p{patient_id}.csv\")                     # Patient Filepath from detailed Folder\n",
    "        mask_filepath = Path(f\"{self.mask_folderpath}/p{patient_id}.nii\")                           # Mask Filepath from detailed Folder\n",
    "        assert(patient_filepath.exists()                                                            # Assertion for the Existence of Patient File in said Folder\n",
    "        ), f\"Filepath for Patient {patient_id} is not in the Dataset!\"\n",
    "        assert(mask_filepath.exists()                                                               # Assertion for the Existence of Mask File in said Folder\n",
    "        ), f\"Filepath for Mask {patient_id} is not in the Dataset!\"\n",
    "        file_size = os.path.getsize(patient_filepath)                                               # Memory Space occupied by Patient File\n",
    "        mask_size = os.path.getsize(mask_filepath)                                                  # Memory Space occupied by Mask File\n",
    "        available_memory = psutil.virtual_memory().available                                        # Memory Space Available for Computation\n",
    "        assert(available_memory >= (file_size + mask_size)                                          # Assertion for the Existence of Available Memory Space\n",
    "        ), f\"ERROR: Dataset requires {file_size + mask_size}b, but only {available_memory}b is available!\"\n",
    "        pX = pd.read_csv(patient_filepath); del pX['Unnamed: 0']                                    # Full Patient Data\n",
    "        pMask = load_img(mask_filepath)                                                             # Patient Mask Data\n",
    "        pX = unmask(pX, pMask); pX = pX.get_fdata()                                                 # Unmasking of Full Patient Data\n",
    "        pX = np.transpose(pX, (3, 2, 0, 1))                                                         # Full Patient Data Reshapping\n",
    "        return pX\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Pre-Processing Alternative Method: Interpolation\n",
    "    def prep_interpolation(\n",
    "        self,\n",
    "        data: np.array,\n",
    "        final_shape: np.array = np.array((85, 92, 56)),\n",
    "    ):\n",
    "\n",
    "        # Input Data Assertions\n",
    "        assert(data.ndim >= 3), \"ERROR: Pre-Processing Input Data has the Wrong Dimmensions\"\n",
    "        assert(len(final_shape) == 3), \"ERROR: Pre-Processing Output has the Wrong Dimmensions\"\n",
    "\n",
    "        #\n",
    "        final_data = np.empty((data.shape[0], final_shape[0], final_shape[1], final_shape[2]))\n",
    "        ratio = np.divide(data.shape[1::], final_shape)\n",
    "        for sample, x, y, z in itertools.product(   range(data.shape[0]),\n",
    "                                                    range(final_shape[0]),\n",
    "                                                    range(final_shape[1]),\n",
    "                                                    range(final_shape[2])):\n",
    "            final_data[sample][x][y][z] = data[sample][int(x * ratio[0])][int(y * ratio[1])][int(z * ratio[2])]\n",
    "        return final_data\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Pre-Processing Alternative Method: Zero-Padding\n",
    "    def prep_zeroPadding(\n",
    "        data: np.array,\n",
    "        final_shape: np.array = np.array((83, 92, 56)),\n",
    "        #final_shape: np.array = np.array((90, 100, 60)),\n",
    "    ):\n",
    "\n",
    "        # Input Data Assertions\n",
    "        assert(data.ndim >= 3), \"ERROR: Pre-Processing Input Data has the Wrong Dimmensions\"\n",
    "        assert(len(final_shape) == 3), \"ERROR: Pre-Processing Output has the Wrong Dimmensions\"\n",
    "        assert(len(np.where((final_shape >= data.shape[1::]) == False)[0]) == 0\n",
    "        ), \"ERROR: Pre-Processed Output Data Shape < Original Input Data Shape\"\n",
    "\n",
    "        # Zero-Padding Implementation\n",
    "        print(np.subtract(final_shape, data.shape[1::]))\n",
    "        padding = (np.hstack((0, np.subtract(final_shape, data.shape[1::]))) / 2).astype(np.float32)\n",
    "        padding = padding.reshape((1, -1)).T + np.array([0, 0])\n",
    "        padding[:, 0] = np.ceil(padding[:, 0]); padding[:, 1] = np.floor(padding[:, 1])\n",
    "        final_data = np.pad(data, padding.astype(np.int32), 'constant')\n",
    "        return final_data\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Pre-Processing Alternative Method: Convolutional Layer\n",
    "    \"\"\"\n",
    "    class preProcess(nn.Module):\n",
    "\n",
    "        # Constructor / Initialization Function\n",
    "        def __init__(\n",
    "            self,\n",
    "            data: pd.DataFrame,\n",
    "            pre_shape: int = 512,\n",
    "        ):\n",
    "\n",
    "            # Parameter Value Access\n",
    "            super(preProcess).__init__()\n",
    "            self.data = data.T\n",
    "            assert(data.ndim == 4), \"ERROR: Input Image Shape not Supported! (4D Arrays only)\"\n",
    "            assert(self.pre_shape < (data.shape[1] * data.shape[2] * data.shape[3])\n",
    "            ), \"ERROR: Convolution Layer Size must be smaller than Original Image's no. of Voxels!\"\n",
    "\n",
    "            # Convolutional Layer Structure\n",
    "            print(self.data.shape)\n",
    "            out = self.conv_layer(data.shape[0], )\n",
    "            print(out.shape)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Convolutional Layer \n",
    "        def conv_layer(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels:int,\n",
    "        ):\n",
    "\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(  in_channels, out_channels,\n",
    "                            kernel_size = (3, 3, 3),\n",
    "                            padding = 0),\n",
    "                nn.MaxPool3d((2, 2, 2)),\n",
    "                nn.Dropout(p = 0.15), )\n",
    "                #nn.LeakyReLU(),\n",
    "                #nn.MaxPool3d((2, 2, 2)),)\n",
    "    \"\"\"\n",
    "        \n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # 2D Conversion Functionality\n",
    "    def slice_(\n",
    "        self,\n",
    "        data: np.array,\n",
    "        interval: np.array = [10, 45],\n",
    "    ):\n",
    "\n",
    "        # Slice Selection & 3D to 2D Conversion\n",
    "        assert(data.ndim >= 3), \"ERROR: Pre-Processing Input Data has the Wrong Dimmensions\"\n",
    "        assert(len(interval) == 2), \"ERROR: Slice Interval has the Wrong Dimmensions\"\n",
    "        interval = range(interval[0], interval[1]); data = data[:, interval, :, :]\n",
    "        data = data.reshape((data.shape[0] * data.shape[1], data.shape[2], data.shape[3]))\n",
    "        return data\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Patient Data Splitting Function\n",
    "    def split_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "        train_params: int = 500,            # Number / Percentage of Parameters to be used in the Training Section of the Patient\n",
    "        percentage: bool = False,           # Control Variable for the Usage of Percentage Values in train_params\n",
    "        sample_shuffle: bool = False,       # Ability to Shuffle the Samples inside both Training and Validation Datasets\n",
    "    ):\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(percentage):\n",
    "            assert(0 < train_params <= 100                              # Percentage Limits for Number of Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            train_params = train_params / 100                           # Percentage Value for Training Parameters\n",
    "            val_params = 1 - train_params                               # Percentage Value for Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Numerical Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params                  # Numerical Limits for Number of Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            val_params = self.num_params - train_params                 # Numerical Value for Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Patient Data Access & Pre-Processing\n",
    "        pX = self.get_patient(patient_number)                                                   # Patient Data Access\n",
    "        py = self.params; py['Patient'] = self.patient_info['Patient'].iloc[patient_number]     # Patient Data Label Handling\n",
    "        if(self.pre_processing == 'Zero Padding'): pX = self.prep_zeroPadding(pX)               # Zero Padding Pre-Processing\n",
    "        elif(self.pre_processing == 'Interpolation'): pX = self.prep_interpolation(pX)          # Interpolation Pre-Processing\n",
    "        #elif(self.pre_processing == 'CNN'): pX = self.prep_cnn(pX)                             # CNN Pre-Processing\n",
    "\n",
    "        # Patient Dataset Splitting into Training & Validation Sets\n",
    "        if(self.slice): pX = self.slice_(pX)\n",
    "        pX_train, pX_val, py_train, py_val = train_test_split(  pX, py,\n",
    "                                                                test_size = val_params,\n",
    "                                                                shuffle = sample_shuffle,\n",
    "                                                                random_state = 42)\n",
    "        return pX_train, pX_val, py_train, py_val\n",
    "        \n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Dataset Splitting Function\n",
    "    def split(\n",
    "        self,\n",
    "        test_patients: int = 1,                     # Number of Patients to be used in the Test Set\n",
    "        train_params: int = 500,                    # Number / Percentage of Parameters for the Training of the Training Set\n",
    "        test_params: int = 20,                      # Number / Percentage of Parameters for the Training of the Test Set\n",
    "        pre_processing: str = 'Zero Padding',       # Control Variable for the Choice of Pre-Processing Method\n",
    "        #slice: bool = True,                        # Control Variable for the 2D Conversion of the 3D Dataset\n",
    "        percentage: bool = False,                   # Control Variable for the Usage of Percentage Values in train_params\n",
    "        patient_shuffle: bool = False,              # Ability to Shuffle the Patients that compose both Training / Validation and Test Datasets\n",
    "        sample_shuffle: bool = False,               # Ability to Shuffle the Samples inside both Training / Validation and Test Datasets\n",
    "    ):\n",
    "\n",
    "        # Patient Number Variable Logging\n",
    "        assert(0 < test_patients <= self.num_patients               # Limits for Number of Test Set Patients\n",
    "        ), f\"ERROR: Test Patient Number not Supported!\"\n",
    "        assert(pre_processing == 'Zero Padding' or pre_processing == 'Interpolation' or pre_processing == 'CNN'\n",
    "        ), \"ERROR: Pre-Processing Method not Supported!\"\n",
    "        self.train_patients = self.num_patients - test_patients     # Number of Patients to be used in the Training & Validation Sets\n",
    "        self.test_patients = test_patients                          # Number of Patients to be used in the Test Sets\n",
    "        self.pre_processing = pre_processing                        # Chosen Pre-Processing Method\n",
    "\n",
    "        # Patient Shuffling Feature\n",
    "        if(patient_shuffle): self.patient_info = self.patient_info.iloc[np.random.permutation(len(self.patient_info))]\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(percentage):\n",
    "            assert(0 < train_params <= 100                              # Percentage Limits for Number of Training Set's Parameters\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"\n",
    "            self.trainTrain_params = train_params                       # Percentage Value for Training Set's Training Parameters\n",
    "            self.trainVal_params = 100 - train_params                   # Percentage Value for Training Set's Validation Parameters\n",
    "            assert(0 < test_params <= 100                               # Percentage Limits for Number of Test Set's Parameters\n",
    "            ), f\"ERROR: Test Set's Parameter Number not Supported!\"\n",
    "            self.testTrain_params = test_params                         # Percentage Value for Test Set's Training Parameters\n",
    "            self.testVal_params = 100 - test_params                     # Percentage Value for Test Set's Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params                  # Numerical Limits for Number of Training Set's Parameters\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"\n",
    "            self.trainTrain_params = train_params                       # Numerical Value for Training Set's Training Parameters\n",
    "            self.trainVal_params = self.num_params - train_params       # Numerical Value for Training Set's Validation Parameters\n",
    "            assert(0 < test_params <= self.num_params                   # Numerical Limits for Number of Test Set's Parameters\n",
    "            ), f\"ERROR: Test Set's Parameter Number not Supported!\"\n",
    "            self.testTrain_params = test_params                         # Numerical Value for Test Set's Training Parameters\n",
    "            self.testVal_params = self.num_params - test_params         # Numerical Value for Test Set's Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Training & Validation Sets Building\n",
    "        self.train_set = dict.fromkeys(('X_train', 'X_val', 'y_train', 'y_val'))\n",
    "        X_train = np.empty([0, self.pre_shape]); X_val = np.empty([0, self.pre_shape])\n",
    "        y_train = np.empty([0, 7]); y_val = np.empty([0, 7])\n",
    "        for p in range(self.train_patients):\n",
    "\n",
    "            # Training Patient Data Access & Treatment\n",
    "            print(f\"Adding Patient {self.patient_info['Patient'].iloc[p]}'s Data to the Training Set...\")       # Display of the Patient being Added to the Test Set\n",
    "            pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                    train_params = self.trainTrain_params,\n",
    "                                                                    percentage = percentage,\n",
    "                                                                    sample_shuffle = sample_shuffle)\n",
    "            X_train = np.concatenate((X_train, pX_train), axis = 0); X_val = np.concatenate((X_val, pX_val), axis = 0)\n",
    "            y_train = np.concatenate((y_train, py_train), axis = 0); y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "        \n",
    "        self.train_set['X_train'] = pd.DataFrame(X_train); self.train_set['X_val'] = pd.DataFrame(X_val)\n",
    "        self.train_set['y_train'] = pd.DataFrame(y_train); self.train_set['y_val'] = pd.DataFrame(y_val)\n",
    "        del X_train, X_val, y_train, y_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Test Set Building\n",
    "        self.test_set = dict.fromkeys(('X_train', 'X_val', 'y_train', 'y_val'))                             # Creation of Empty Dictionary to Fit Patient Data\n",
    "        X_train = np.empty([0, self.pre_shape]); X_val = np.empty([0, self.pre_shape])\n",
    "        y_train = np.empty([0, 7]); y_val = np.empty([0, 7])\n",
    "        for p in range(self.train_patients, self.train_patients + self.test_patients):\n",
    "\n",
    "            # Training Patient Data Access & Treatment\n",
    "            print(f\"Adding Patient {self.patient_info['Patient'].iloc[p]}'s Data to the Test Set...\")       # Display of the Patient being Added to the Test Set\n",
    "            pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                    train_params = self.testTrain_params,\n",
    "                                                                    percentage = percentage,\n",
    "                                                                    sample_shuffle = sample_shuffle)\n",
    "            X_train = np.concatenate((X_train, pX_train), axis = 0); X_val = np.concatenate((X_val, pX_val), axis = 0)\n",
    "            y_train = np.concatenate((y_train, py_train), axis = 0); y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "        \n",
    "        self.test_set['X_train'] = pd.DataFrame(X_train); self.test_set['X_val'] = pd.DataFrame(X_val)\n",
    "        self.test_set['y_train'] = pd.DataFrame(y_train); self.test_set['y_val'] = pd.DataFrame(y_val)\n",
    "        del X_train, X_val, y_train, y_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Split Datasets' Content Report\n",
    "        if(percentage):\n",
    "            print(tabulate([[self.train_patients, f\"{(self.trainTrain_params / 100) * self.num_params} ({self.trainTrain_params}%)\", f\"{(self.trainVal_params / 100) * self.num_params} ({self.trainVal_params}%)\"],\n",
    "                            [self.test_patients, f\"{(self.testTrain_params / 100) * self.num_params} ({self.testTrain_params}%)\", f\"{(self.testVal_params / 100) * self.num_params} ({self.testVal_params}%)\"]],\n",
    "                            headers = ['No. Patients', 'Training Parameters', 'Validation Parameters'],\n",
    "                            showindex = ['Training Set', 'Test Set'], tablefmt = 'fancy_grid'))\n",
    "        else:\n",
    "            print(tabulate([[self.train_patients, f\"{self.trainTrain_params} ({np.round((self.trainTrain_params / self.num_params) * 100, 2)}%)\", f\"{self.trainVal_params} ({np.round((self.trainVal_params / self.num_params) * 100, 2)}%)\"],\n",
    "                            [self.test_patients, f\"{self.testTrain_params} ({np.round(self.testTrain_params / self.num_params, 2)}%)\", f\"{self.testVal_params} ({np.round(self.testVal_params / self.num_params, 2)}%)\"]],\n",
    "                            headers = ['No. Patients', 'Training Parameters', 'Validation Parameters'],\n",
    "                            showindex = ['Training Set', 'Test Set'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "    \n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Dataset Saving Function\n",
    "    def save(\n",
    "        self,\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Vertical 3D MUDI (Version {version})', 'wb')\n",
    "        pickle.dump(self, f)\n",
    "        f.close\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loading Function\n",
    "    def load(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Vertical 3D MUDI (Version {version})', 'rb')\n",
    "        mudi = pickle.load(f)\n",
    "        f.close\n",
    "        return mudi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Initialization Example\n",
    "mudi = v3DMUDI( parse.patient_folderpath,\n",
    "                parse.mask_folderpath,\n",
    "                parse.param_filepath,\n",
    "                parse.info_filepath)\n",
    "#mudi.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "mudi = v3DMUDI.load(Path(f\"{main_folderpath}Saved Data\"), version = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7f3519d05e42ae87de943e21fdbc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Sample', max=1343), IntSlider(v"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3D Interactive Plotting Function\n",
    "def plot(\n",
    "    sample_number,\n",
    "    slice_number,\n",
    "):\n",
    "\n",
    "    # Patient Sample & Slice for Visualization\n",
    "    img = data[sample_number]\n",
    "    img = rotate(img[slice_number].T, angle = 180)\n",
    "    #img = data[slice_number, :, :, sample_number].T\n",
    "    plt.figure(figsize = (10, 20)); plt.imshow(img, cmap = 'gray'); plt.axis('off')\n",
    "    plt.title(f\"Patient #{patient_number} | Sample #{sample_number} | Slice #{slice_number}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Patient Data Visualization Function\n",
    "patient_number = 0; data = mudi.get_patient(patient_number)\n",
    "sample_slider = IntSlider(value = 0, min = 0, max = data.shape[0] - 1, description = 'Sample', continuous_update = False)\n",
    "slice_slider = IntSlider(value = 0, min = 0, max = data.shape[1] - 1, description = 'Slice', continuous_update = False)\n",
    "interactive(plot, sample_number = sample_slider, slice_number = slice_slider)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7e2413ca9464f5b18ee008ec75e3890212b75ca17b4a3699f34f03bf3acaeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
