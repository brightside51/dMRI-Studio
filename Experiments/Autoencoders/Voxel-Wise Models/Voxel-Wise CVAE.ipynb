{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *Initial* **Setup**\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Library** *Settings*\n",
    "\n",
    "The Real Package Name must be found in https://pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\callbacks\\data_monitor.py:20: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"wandb\")\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\datamodules\\experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import gc\n",
    "import math\n",
    "import pickle\n",
    "import psutil\n",
    "import nilearn\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import pl_bolts                     # lightning_bolts\n",
    "import tensorboard\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import fvcore\n",
    "import matplotlib.pyplot as plt\n",
    "import itk\n",
    "import itkwidgets\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "import alive_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionality Import\n",
    "from pathlib import Path\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "from typing import List, Literal, Optional, Callable, Dict, Literal, Optional, Union, Tuple, Iterable\n",
    "from collections import OrderedDict, namedtuple\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pl_bolts.models.autoencoders.components import resnet18_encoder, resnet18_decoder\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Layer, Softmax, Input\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import Constant, glorot_normal\n",
    "from keras.optimizers import Adam\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "\n",
    "from nilearn.image import load_img\n",
    "from nilearn.masking import unmask\n",
    "from PIL import Image\n",
    "from ipywidgets import interactive, IntSlider\n",
    "from tabulate import tabulate\n",
    "from alive_progress import alive_bar\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Control** *Station*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parametrizations Parser Initialization\n",
    "data_parser = argparse.ArgumentParser(\n",
    "        description = \"1D MUDI Dataset Settings\")\n",
    "data_parser.add_argument(                               # Dataset Version Variable\n",
    "        '--version', type = int,                        # Default: 0\n",
    "        default = 2,\n",
    "        help = \"Dataset Save Version\")\n",
    "data_parser.add_argument(                               # Control Variable for the Usage of only 1 Voxel per Sample\n",
    "        '--conversion', type = bool,                    # Default: True\n",
    "        default = True,\n",
    "        help = \"Control Variable for the Usage of only 1 Voxel per Sample\")\n",
    "data_parser.add_argument(                               # Dataset Batch Size Value\n",
    "        '--batch_size', type = int,                     # Default: 500\n",
    "        default = 256,\n",
    "        help = \"Dataset Batch Size Value\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset Label Parametrization Arguments\n",
    "data_parser.add_argument(                       # Control Variable for the Inclusion of Patient ID in Labels\n",
    "        '--patient_id', type = bool,            # Default: True\n",
    "        default = False,\n",
    "        help = \"Control Variable for the Inclusion of Patient ID in Labels\")\n",
    "data_parser.add_argument(                       # Control Variable for the Conversion of 3 Gradient Directions\n",
    "        '--gradient_coord', type = bool,        # Coordinates into 2 Gradient Direction Angles (suggested by prof. Chantal)\n",
    "        default = False,                        # Default: True (3 Coordinate Gradient Values)\n",
    "        help = \"Control Variable for the Conversion of Gradient Direction Mode\")\n",
    "data_parser.add_argument(                       # Control Variable for the Rescaling & Normalization of Labels\n",
    "        '--label_norm', type = bool,            # Default: True\n",
    "        default = True,\n",
    "        help = \"Control Variable for the Rescaling & Normalization of Labels\")\n",
    "data_settings = data_parser.parse_args(\"\")\n",
    "num_labels = 7\n",
    "if not(data_settings.patient_id): num_labels -= 1           # Exclusion of Patiend ID\n",
    "if not(data_settings.gradient_coord): num_labels -= 1       # Conversion of Gradient Coordinates to Angles\n",
    "data_parser.add_argument(                                   # Dataset Number of Labels\n",
    "        '--num_labels', type = int,                         # Default: 7\n",
    "        default = num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of File & Folderpath Arguments\n",
    "data_parser.add_argument(                               # Path for Main Dataset Folder\n",
    "        '--main_folderpath', type = str,\n",
    "        default = '../../../Datasets/MUDI Dataset',\n",
    "        help = 'Main Folderpath for Root Dataset')\n",
    "data_settings = data_parser.parse_args(\"\")\n",
    "data_parser.add_argument(                               # Path for Parameter Value File\n",
    "        '--param_filepath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Raw Data/parameters_new.xlsx'),\n",
    "        help = 'Input Filepath for Parameter Value Table')\n",
    "data_parser.add_argument(                               # Path for Parameter Value File\n",
    "        '--data_filepath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Raw Data/data_.hdf5'),\n",
    "        help = 'Input Filepath for Parameter Value Table')\n",
    "data_parser.add_argument(                               # Path for Patient Information File\n",
    "        '--info_filepath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Raw Data/header1_.csv'),\n",
    "        help = 'Input Filepath for Patient Information Table')\n",
    "data_parser.add_argument(                               # Path for Folder Containing Patient Data Files\n",
    "        '--patient_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Patient Data'),\n",
    "        help = 'Input Folderpath for Segregated Patient Data')\n",
    "data_parser.add_argument(                               # Path for Folder Containing Mask Data Files\n",
    "        '--mask_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Patient Mask'),\n",
    "        help = 'Input Folderpath for Segregated Patient Mask Data')\n",
    "data_parser.add_argument(                               # Path for Dataset Saved Files\n",
    "        '--save_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Output Folderpath for MUDI Dataset Saved Versions')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset Splitting Arguments\n",
    "data_parser.add_argument(                       # Number of Patients to be used in the Test Set\n",
    "        '--test_patients', type = int,          # Default: 1\n",
    "        default = 1,\n",
    "        help = \"Number of Patients in Test Set\")\n",
    "data_parser.add_argument(                       # Number / Percentage of Parameters for Training Set's Training\n",
    "        '--train_params', type = int,           # Default: 500\n",
    "        default = 500,\n",
    "        help = \"Number / Percentage of Patients in the Training of the Training Set\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Boolean Control Input & Shuffling Arguments\n",
    "data_parser.add_argument(                       # Control Variable for the Usage of Percentage Values in Parameters\n",
    "        '--percentage', type = bool,            # Default: False\n",
    "        default = False,\n",
    "        help = \"Control Variable for the Usage of Percentage Values in Parameters\")\n",
    "data_parser.add_argument(                       # Ability to Shuffle the Patients that compose both Training and Test Sets\n",
    "        '--patient_shuffle', type = bool,       # Default: False\n",
    "        default = False,\n",
    "        help = \"Ability to Shuffle the Patients that compose both Training and Test Sets\")\n",
    "data_parser.add_argument(                       # Ability to Shuffle the Samples inside both Training and Validation Sets\n",
    "        '--sample_shuffle', type = bool,        # Default: False\n",
    "        default = False,\n",
    "        help = \"Ability to Shuffle the Samples inside both Training and Validation Sets\")\n",
    "data_parser.add_argument(                       # Number of Workers for DataLoader Usage\n",
    "        '--num_workers', type = int,                # Default: 1\n",
    "        default = 20,\n",
    "        help = \"Number of Workers for DataLoader Usage\")\n",
    "data_settings = data_parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Wise CVAE Model Parametrizations Parser Initialization\n",
    "model_parser = argparse.ArgumentParser(\n",
    "        description = \"Voxel-Wise CVAE Settings\")\n",
    "model_parser.add_argument(              # Model Version Variable\n",
    "        '--model_version', type = int,  # Default: 0\n",
    "        default = 2,\n",
    "        help = \"Experiment Version\")\n",
    "model_parser.add_argument(              # Dataset Version Variable\n",
    "        '--data_version', type = int,   # Default: 0\n",
    "        default = 0,\n",
    "        help = \"MUDI Dataset Version\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Filepath Arguments\n",
    "model_parser.add_argument(\n",
    "        '--reader_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Dataset Reader'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Reader')\n",
    "model_parser.add_argument(\n",
    "        '--data_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Saved Versions')\n",
    "model_parser.add_argument(\n",
    "        '--model_folderpath', type = str,\n",
    "        default = 'Model Builds',\n",
    "        help = 'Input Folderpath for Model Build & Architecture')\n",
    "model_parser.add_argument(\n",
    "        '--script_folderpath', type = str,\n",
    "        default = 'Training Scripts',\n",
    "        help = 'Input Folderpath for Training & Testing Script Functions')\n",
    "model_parser.add_argument(\n",
    "        '--save_folderpath', type = str,\n",
    "        default = 'Saved Models',\n",
    "        help = 'Output Folderpath for Saved & Saving Models')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Training Requirement Arguments\n",
    "model_parser.add_argument(              # Number of Epochs\n",
    "        '--num_epochs', type = int,     # Default: 1\n",
    "        default = 50,\n",
    "        help = \"Number of Epochs in Training Mode\")\n",
    "model_parser.add_argument(              # Base Learning Rate\n",
    "        '--base_lr', type = float,      # Default: 1e-4\n",
    "        default = 1e-3,\n",
    "        help = \"Base Learning Rate Value in Training Mode\")\n",
    "model_parser.add_argument(              # Weight Decay Value\n",
    "        '--weight_decay', type = float, # Default: 0.0001\n",
    "        default = 1e-4,\n",
    "        help = \"Weight Decay Value in Training Mode\")\n",
    "model_parser.add_argument(              # Learning Rate Decay Ratio\n",
    "        '--lr_decay', type = float,     # Default: 0.9\n",
    "        default = 0.9,\n",
    "        help = \"Learning Rate Decay Value in Training Mode\")\n",
    "        \n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Model Architecture Arguments\n",
    "model_parser.add_argument(              # Dataset Number of Labels\n",
    "        '--num_labels', type = int,     # Default: 7\n",
    "        default = data_settings.num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "model_parser.add_argument(              # Latent Space Dimensionality\n",
    "        '--latent_dim', type = int,     # Default: 128\n",
    "        default = 128,\n",
    "        help = \"Latent Space Dimensionality Value\")\n",
    "model_parser.add_argument(              # Number of Reference Channels for Encoder Architecture\n",
    "        '--num_channel', type = int,    # Default: 64\n",
    "        default = 64,\n",
    "        help = \"Number of Reference Channels for Encoder Architecture\")\n",
    "model_parser.add_argument(              # Number of Main Layers in Encoder & Decoder Architectures\n",
    "        '--num_layers', type = int,     # Default: 3\n",
    "        default = 3,\n",
    "        help = \"Number of Main Layers in Encoder & Decoder Architectures\")\n",
    "\"\"\"\n",
    "model_parser.add_argument(              # Convolutional Layer Expansion Value\n",
    "        '--expansion', type = int,      # Default: 1\n",
    "        default = 1,\n",
    "        help = \"Convolutional Layer Expansion Value\")\n",
    "model_parser.add_argument(              # Kullback-Leibler Loss Weight\n",
    "        '--kl_alpha', type = int,       # Default: 1\n",
    "        default = 1,\n",
    "        help = \"Kullback-Leibler Loss Weight\")\n",
    "if data_settings.dim == 2: num_channel = 1\n",
    "else: num_channel = data_settings.num_slices\n",
    "model_parser.add_argument(              # Number of Channels in given 3D Image Batch\n",
    "        '--num_channel', type = int,    # Default: 1 (for 2D MUDI Dataset)\n",
    "        default = num_channel,\n",
    "        help = \"Number of Channels in given 3D Image Batch\")\n",
    "model_parser.add_argument(              # Image Side Length (No Support for non-Square Images)\n",
    "        '--img_shape', type = int,      # Default: 128 \n",
    "        default = data_settings.img_shape[-1],\n",
    "        help = \"Image Side Length\")\n",
    "\"\"\"\n",
    "\n",
    "model_settings = model_parser.parse_args(\"\")\n",
    "model_settings.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Wise Conditional Linear Neural Network Model Parametrizations Parser Initialization\n",
    "model_parser = argparse.ArgumentParser(\n",
    "        description = \"Voxel-Wise clNN Settings\")\n",
    "model_parser.add_argument(              # Model Version Variable\n",
    "        '--model_version', type = int,  # Default: 0\n",
    "        default = 4,\n",
    "        help = \"Experiment Version\")\n",
    "model_parser.add_argument(              # Dataset Version Variable\n",
    "        '--data_version', type = int,   # Default: 0\n",
    "        default = 0,\n",
    "        help = \"MUDI Dataset Version\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Filepath Arguments\n",
    "model_parser.add_argument(\n",
    "        '--reader_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Dataset Reader'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Reader')\n",
    "model_parser.add_argument(\n",
    "        '--data_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Saved Versions')\n",
    "model_parser.add_argument(\n",
    "        '--model_folderpath', type = str,\n",
    "        default = 'Model Builds',\n",
    "        help = 'Input Folderpath for Model Build & Architecture')\n",
    "model_parser.add_argument(\n",
    "        '--script_folderpath', type = str,\n",
    "        default = 'Training Scripts',\n",
    "        help = 'Input Folderpath for Training & Testing Script Functions')\n",
    "model_parser.add_argument(\n",
    "        '--save_folderpath', type = str,\n",
    "        default = 'Saved Models',\n",
    "        help = 'Output Folderpath for Saved & Saving Models')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Training Requirement Arguments\n",
    "model_parser.add_argument(              # Number of Epochs\n",
    "        '--num_epochs', type = int,     # Default: 1\n",
    "        default = 50,\n",
    "        help = \"Number of Epochs in Training Mode\")\n",
    "model_parser.add_argument(              # Base Learning Rate\n",
    "        '--base_lr', type = float,      # Default: 1e-4\n",
    "        default = 1e-3,\n",
    "        help = \"Base Learning Rate Value in Training Mode\")\n",
    "model_parser.add_argument(              # Weight Decay Value\n",
    "        '--weight_decay', type = float, # Default: 0.0001\n",
    "        default = 1e-4,\n",
    "        help = \"Weight Decay Value in Training Mode\")\n",
    "model_parser.add_argument(              # Learning Rate Decay Ratio\n",
    "        '--lr_decay', type = float,     # Default: 0.9\n",
    "        default = 0.9,\n",
    "        help = \"Learning Rate Decay Value in Training Mode\")\n",
    "        \n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Model Architecture Arguments\n",
    "model_parser.add_argument(              # Dataset Number of Labels\n",
    "        '--num_labels', type = int,     # Default: 7\n",
    "        default = data_settings.num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "model_parser.add_argument(              # Input Parameters\n",
    "        '--in_params', type = int,      # Default: 500\n",
    "        default = 500,\n",
    "        help = \"Input Parameters\")\n",
    "model_parser.add_argument(              # Output Parameters\n",
    "        '--out_params', type = int,     # Default: 1344\n",
    "        default = 1344,\n",
    "        help = \"Output Parameters\")\n",
    "model_parser.add_argument(              # Latent Space Dimensionality\n",
    "        '--latent_dim', type = int,     # Default: 128\n",
    "        default = 128,\n",
    "        help = \"Latent Space Dimensionality Value\")\n",
    "model_parser.add_argument(              # Number of Decoder Hidden Layers\n",
    "        '--num_hidden', type = int,    # Default: 64\n",
    "        default = 2,\n",
    "        help = \"Number of Decoder Hidden Layers\")\n",
    "model_parser.add_argument(              # Leaky ReLU Negative Slope Value\n",
    "        '--neg_slope', type = float,    # Default: 0.2\n",
    "        default = 0.2,\n",
    "        help = \"Leaky ReLU Negative Slope Value\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Buffer Parameter Arguments\n",
    "model_parser.add_argument(                      # Maximum Temperature for Gumble Softmax\n",
    "        '--max_temp', type = float,             # Default: 10\n",
    "        default = 10.0,\n",
    "        help = \"Maximum Temperature for Gumble Softmax\")\n",
    "model_parser.add_argument(                      # Minimum Temperature for Gumble Softmax\n",
    "        '--min_temp', type = float,             # Default: 0.1\n",
    "        default = 0.1,\n",
    "        help = \"Minimum Temperature for Gumble Softmax\")\n",
    "model_parser.add_argument(                      # Regularization Epsilon\n",
    "        '--reg_eps', type = float,              # Default: 1e-10\n",
    "        default = 1e-10,\n",
    "        help = \"Regularization Epsilon - Minimum Value for the Clamped Softmax in Regularization\")\n",
    "model_parser.add_argument(                      # Regularization Threshold\n",
    "        '--reg_threshold', type = float,        # Default: 3.0\n",
    "        default = 1.0,\n",
    "        help = \"Regularization Threshold - Value for which the Sum of Neuron Probabilities will mean the Penalization of the Encoder\")\n",
    "model_parser.add_argument(                      # Regularization Lambda\n",
    "        '--reg_lambda', type = float,           # Default: 0 (No Regularization)\n",
    "        default = 0.0,\n",
    "        help = \"Regularization Lambda - Regularization Term Weight\")\n",
    "\n",
    "model_settings = model_parser.parse_args(\"\")\n",
    "model_settings.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Neural Network Model Parametrizations Parser Initialization\n",
    "model_parser = argparse.ArgumentParser(\n",
    "        description = \"fcNN Settings\")\n",
    "model_parser.add_argument(              # Model Version Variable\n",
    "        '--model_version', type = int,  # Default: 0\n",
    "        default = 0,\n",
    "        help = \"Experiment Version\")\n",
    "model_parser.add_argument(              # Dataset Version Variable\n",
    "        '--data_version', type = int,   # Default: 0\n",
    "        default = 1,\n",
    "        help = \"MUDI Dataset Version\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Filepath Arguments\n",
    "model_parser.add_argument(\n",
    "        '--reader_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Dataset Reader'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Reader')\n",
    "model_parser.add_argument(\n",
    "        '--data_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Saved Versions')\n",
    "model_parser.add_argument(\n",
    "        '--model_folderpath', type = str,\n",
    "        default = 'Model Builds',\n",
    "        help = 'Input Folderpath for Model Build & Architecture')\n",
    "model_parser.add_argument(\n",
    "        '--script_folderpath', type = str,\n",
    "        default = 'Training Scripts',\n",
    "        help = 'Input Folderpath for Training & Testing Script Functions')\n",
    "model_parser.add_argument(\n",
    "        '--save_folderpath', type = str,\n",
    "        default = 'Saved Models',\n",
    "        help = 'Output Folderpath for Saved & Saving Models')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Training Requirement Arguments\n",
    "model_parser.add_argument(              # Number of Epochs\n",
    "        '--num_epochs', type = int,     # Default: 1\n",
    "        default = 50,\n",
    "        help = \"Number of Epochs in Training Mode\")\n",
    "model_parser.add_argument(              # Base Learning Rate\n",
    "        '--base_lr', type = float,      # Default: 1e-3\n",
    "        default = 1e-3,\n",
    "        help = \"Base Learning Rate Value in Training Mode\")\n",
    "model_parser.add_argument(              # Weight Decay Value\n",
    "        '--weight_decay', type = float, # Default: 1e-4\n",
    "        default = 1e-4,\n",
    "        help = \"Weight Decay Value in Training Mode\")\n",
    "model_parser.add_argument(              # Learning Rate Decay Ratio\n",
    "        '--lr_decay', type = float,     # Default: 0.9\n",
    "        default = 0.9,\n",
    "        help = \"Learning Rate Decay Value in Training Mode\")\n",
    "        \n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Model Architecture Arguments\n",
    "model_parser.add_argument(              # Dataset Number of Labels\n",
    "        '--num_labels', type = int,     # Default: 7\n",
    "        default = data_settings.num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "model_parser.add_argument(              # Number of Parameter Settings for Training\n",
    "        '--in_params', type = int,      # Default: 500\n",
    "        default = data_settings.train_params,\n",
    "        help = \"Number of Parameter Settings for Training\")\n",
    "model_parser.add_argument(              # Total Number of Parameter Settings\n",
    "        '--out_params', type = int,     # Default: 1344\n",
    "        default = 1344,\n",
    "        help = \"Total Number of Parameter Settings\")\n",
    "model_parser.add_argument(              # Number of Hidden Layers in Neural Network\n",
    "        '--num_hidden', type = int,     # Default: 2\n",
    "        default = 2,\n",
    "        help = \"Number of Hidden Layers in Neural Network\")\n",
    "\n",
    "model_settings = model_parser.parse_args(\"\")\n",
    "model_settings.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Generative Linear Voxel Neural Network Model Parametrizations Parser Initialization\n",
    "model_parser = argparse.ArgumentParser(\n",
    "        description = \"cglVNN Settings\")\n",
    "model_parser.add_argument(              # Model Version Variable\n",
    "        '--model_version', type = int,  # Default: 0\n",
    "        default = 3,\n",
    "        help = \"Experiment Version\")\n",
    "model_parser.add_argument(              # Dataset Version Variable\n",
    "        '--data_version', type = int,   # Default: 0\n",
    "        default = 2,\n",
    "        help = \"MUDI Dataset Version\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Filepath Arguments\n",
    "model_parser.add_argument(\n",
    "        '--reader_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Dataset Reader'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Reader')\n",
    "model_parser.add_argument(\n",
    "        '--data_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Saved Versions')\n",
    "model_parser.add_argument(\n",
    "        '--model_folderpath', type = str,\n",
    "        default = 'Model Builds',\n",
    "        help = 'Input Folderpath for Model Build & Architecture')\n",
    "model_parser.add_argument(\n",
    "        '--script_folderpath', type = str,\n",
    "        default = 'Training Scripts',\n",
    "        help = 'Input Folderpath for Training & Testing Script Functions')\n",
    "model_parser.add_argument(\n",
    "        '--save_folderpath', type = str,\n",
    "        default = 'Saved Models',\n",
    "        help = 'Output Folderpath for Saved & Saving Models')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Training Requirement Arguments\n",
    "model_parser.add_argument(              # Number of Epochs\n",
    "        '--num_epochs', type = int,     # Default: 1\n",
    "        default = 50,\n",
    "        help = \"Number of Epochs in Training Mode\")\n",
    "model_parser.add_argument(              # Base Learning Rate\n",
    "        '--base_lr', type = float,      # Default: 1e-3\n",
    "        default = 1e-3,\n",
    "        help = \"Base Learning Rate Value in Training Mode\")\n",
    "model_parser.add_argument(              # Weight Decay Value\n",
    "        '--weight_decay', type = float, # Default: 1e-4\n",
    "        default = 1e-4,\n",
    "        help = \"Weight Decay Value in Training Mode\")\n",
    "model_parser.add_argument(              # Learning Rate Decay Ratio\n",
    "        '--lr_decay', type = float,     # Default: 0.9\n",
    "        default = 0.9,\n",
    "        help = \"Learning Rate Decay Value in Training Mode\")\n",
    "        \n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Model Architecture Arguments\n",
    "model_parser.add_argument(              # Dataset Number of Labels\n",
    "        '--num_labels', type = int,     # Default: 7\n",
    "        default = data_settings.num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "model_parser.add_argument(              # Number of Hidden Layers in Neural Network\n",
    "        '--num_hidden', type = int,     # Default: 2\n",
    "        default = 2,\n",
    "        help = \"Number of Hidden Layers in Neural Network\")\n",
    "model_parser.add_argument(              # Deviance / Expansion of Hidden Layers\n",
    "        '--var_hidden', type = int,     # Default: 64\n",
    "        default = 64,\n",
    "        help = \"Deviance / Expansion of Hidden Layers\")\n",
    "\n",
    "model_settings = model_parser.parse_args(\"\")\n",
    "model_settings.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Data** *Access*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal 1D MUDI Dataset Initialization Class\n",
    "class h1DMUDI(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "\n",
    "        # Parameter Value Access\n",
    "        super(h1DMUDI).__init__()\n",
    "        self.version = settings.version\n",
    "        self.params = pd.read_excel(settings.param_filepath)    # List of Dataset's Parameters\n",
    "        self.num_params = self.params.shape[0]                  # Total Number of Parameters in Dataset\n",
    "        #assert(self.num_params == self.data.shape[0]), \"ERROR: Number of Parameters is Incoherent\" \n",
    "\n",
    "        # Patient Information Access\n",
    "        self.patient_folderpath = settings.patient_folderpath\n",
    "        self.save_folderpath = settings.save_folderpath\n",
    "        self.patient_info = pd.read_csv(settings.info_filepath)     # List of Patients and Corresponding IDs & Image Sizes inside Full Dataset\n",
    "        self.patient_info = self.patient_info[:-1]                  # Eliminating the Last Row containing Useless Information from the Patient Information\n",
    "        self.num_patients = self.patient_info.shape[0]              # Number of Patients inside Full Dataset\n",
    "        self.progress = False                                       # Control Boolean Value for Progress Saving (Data can only be saved if Split)\n",
    "        \n",
    "        # Dataset Sample & Label Handling Settings\n",
    "        self.num_labels = settings.num_labels\n",
    "        if self.settings.gradient_coord: self.params = self.params.drop(columns = ['Gradient theta', 'Gradient phi'])   # Choosing of Gradient Orientation\n",
    "        else: self.params = self.params.drop(columns = ['Gradient x', 'Gradient y', 'Gradient z'])                      # from 3D Cartesian or Polar Referential\n",
    "        assert(self.params.shape[1] == self.num_labels), \"ERROR: Labels wrongly Deleted!\"\n",
    "        if self.settings.label_norm:                                                                                    # Control Boolean Value for the Normalization of Labels\n",
    "            self.scaler = StandardScaler()\n",
    "            self.params = pd.DataFrame( self.scaler.fit_transform(self.params),\n",
    "                                        columns = self.params.columns)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # --------------------------------- Feature & Label Handling ---------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # 1D Image to 1D Voxel Data Conversion Function\n",
    "    # [num_sample, num_feats] -> [num_sample * num_feats, 1]\n",
    "    def convert(\n",
    "        self,\n",
    "        data: np.ndarray,           # 3D Data Array\n",
    "        label: pd.DataFrame\n",
    "    ):\n",
    "\n",
    "        # Conversion from 1D Image Data to 1D Voxel Data\n",
    "        label = label.iloc[np.repeat(np.arange(len(label)), data.shape[1])]\n",
    "        data = pd.DataFrame(data.to_numpy().reshape((data.shape[0] * data.shape[1], 1)))\n",
    "        return data, label\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Full 3D Patient Image Unmasking\n",
    "    def unmask(\n",
    "        settings: argparse.ArgumentParser,\n",
    "        patient_number: int                     # Number for the Patient File being Read and Acquired (in Order)\n",
    "    ):\n",
    "\n",
    "        # Patient Mask Access\n",
    "        mask_filepath = Path(f\"{settings.mask_folderpath}/p{patient_id}.nii\")               # Mask Filepath from detailed Folder\n",
    "        assert(mask_filepath.exists()                                                       # Assertion for the Existence of Mask File in said Folder\n",
    "        ), f\"Filepath for Mask {patient_id} is not in the Dataset!\"\n",
    "\n",
    "        # Patient Data Access\n",
    "        data = load(settings.save_folderpath, settings.version)\n",
    "        pX = data.get_patient(patient_number)\n",
    "        pX = unmask(pX, pMask); pX = pX.get_fdata()     # Unmasking of Full Patient Data\n",
    "        \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Label Scaler Download & Reverse Transformation\n",
    "    def label_unscale(\n",
    "        path: Path,\n",
    "        version: int,\n",
    "        y: np.array or pd.DataFrame\n",
    "    ):\n",
    "\n",
    "        # Label Scaler Download & Reverse Usage\n",
    "        scaler = torch.load(f\"{path}/1D Label Scaler (V{version}).pkl\")\n",
    "        return scaler.inverse_transform(y)\n",
    "        \n",
    "    ##############################################################################################\n",
    "    # ---------------------------------- Data Access & Splitting ---------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Patient Data Access Function\n",
    "    def get_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "    ):\n",
    "\n",
    "        # Patient Data Information\n",
    "        assert(0 <= patient_number < self.num_patients), f\"ERROR: Input Patient not Found!\"         # Assertion for the Existence of the Requested Patient\n",
    "        patient_id = self.patient_info['Patient'].iloc[patient_number]                              # Patient ID contained within the Patient List\n",
    "        patient_filepath = Path(f\"{self.patient_folderpath}/p{patient_id}.csv\")                     # Patient Filepath from detailed Folder\n",
    "        mask_filepath = Path(f\"{self.mask_folderpath}/p{patient_id}.nii\")                           # Mask Filepath from detailed Folder\n",
    "        \n",
    "        # Patient Data Access Memory Requirements\n",
    "        assert(patient_filepath.exists()                                                            # Assertion for the Existence of Patient File in said Folder\n",
    "        ), f\"Filepath for Patient {patient_id} is not in the Dataset!\"\n",
    "        assert(mask_filepath.exists()                                                               # Assertion for the Existence of Mask File in said Folder\n",
    "        ), f\"Filepath for Mask {patient_id} is not in the Dataset!\"\n",
    "        file_size = os.path.getsize(patient_filepath)                                               # Memory Space occupied by Patient File\n",
    "        mask_size = os.path.getsize(mask_filepath)                                                  # Memory Space occupied by Mask File\n",
    "        available_memory = psutil.virtual_memory().available                                        # Memory Space Available for Computation\n",
    "        assert(available_memory >= (file_size + mask_size)                                          # Assertion for the Existence of Available Memory Space\n",
    "        ), f\"ERROR: Dataset requires {file_size + mask_size}b, but only {available_memory}b is available!\"\n",
    "        \n",
    "        # Patient Data Access\n",
    "        pX = pd.read_csv(patient_filepath); del pX['Unnamed: 0']                                    # Full Patient Data\n",
    "        pMask = load_img(mask_filepath)                                                             # Patient Mask Data\n",
    "        #pX = unmask(pX, pMask); pX = pX.get_fdata()                                                # Unmasking of Full Patient Data\n",
    "        del available_memory, mask_size, file_size\n",
    "        return pX, pMask\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Patient Data Splitting Function\n",
    "    def split_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "        train_params: int = 500,            # Number / Percentage of Parameters to be used in the Training Section of the Patient\n",
    "        percentage: bool = False,           # Control Variable for the Usage of Percentage Values in train_params\n",
    "        sample_shuffle: bool = False,       # Ability to Shuffle the Samples inside both Training and Validation Datasets\n",
    "    ):\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(percentage):\n",
    "            assert(0 < train_params <= 100                              # Percentage Limits for Number of Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            train_params = train_params / 100                           # Percentage Value for Training Parameters\n",
    "            val_params = 1 - train_params                               # Percentage Value for Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Numerical Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params                  # Numerical Limits for Number of Training Parameters\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"\n",
    "            val_params = self.num_params - train_params                 # Numerical Value for Validation Parameters\n",
    "            if self.voxel_wise:                                         # Correction of Vaidation Parameter Number for Voxel-Wise Data\n",
    "                patient_feats = self.patient_info['Voxels'].iloc[patient_number]\n",
    "                val_params *= patient_feats\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Patient Data & Label Access & Handling\n",
    "        pX = self.get_patient(patient_number); py = self.params                                     # Patient Data Access\n",
    "        if self.patient_id: py['Patient'] = self.patient_info['Patient'].iloc[patient_number]       # Patient ID Label Handling\n",
    "        if self.voxel_wise: pX, py = self.convert(pX, py)                                           # 1D Image to Voxel Data Conversion\n",
    "\n",
    "        # Patient Dataset Splitting into Training & Validation Sets\n",
    "        if val_params != 0:\n",
    "            pX_train, pX_val, py_train, py_val = train_test_split(  pX, py,\n",
    "                                                                    test_size = val_params,\n",
    "                                                                    shuffle = sample_shuffle,\n",
    "                                                                    random_state = 42)\n",
    "            return pX_train, pX_val, py_train, py_val\n",
    "        else: return pX, py\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Splitting Function\n",
    "    def split(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "\n",
    "        # Patient Number Variable Logging\n",
    "        assert(0 < settings.test_patients <= self.num_patients      # Limits for Number of Test Set Patients\n",
    "        ), f\"ERROR: Test Patient Number not Supported!\"\n",
    "        self.train_patients = self.num_patients - settings.test_patients    # Number of Patients to be used in the Training Set\n",
    "        self.test_patients = settings.test_patients                         # Number of Patients to be used in the Test Set\n",
    "        self.batch_size = settings.batch_size                               # Sample Batch Size Variable\n",
    "        self.patient_shuffle = settings.patient_shuffle                     # Ability to Shuffle the Patients that compose both Training / Validation and Test Datasets\n",
    "        self.sample_shuffle = settings.sample_shuffle                       # Ability to Shuffle the Samples inside both Training / Validation and Test Datasets\n",
    "        self.num_workers = settings.num_workers                             # Number of Workers in the Usage of DataLoaders\n",
    "        self.progress = True                                                # Control Boolean Value for Progress Saving (Data can only be saved if Split)\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Patient Shuffling Feature\n",
    "        if(self.patient_shuffle): self.patient_info = self.patient_info.iloc[np.random.permutation(len(self.patient_info))]\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(settings.percentage):\n",
    "            assert(0 < settings.train_params <= 100                         # Percentage Limits for Number of Training Set's Parameters\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"\n",
    "            self.trainTrain_params = settings.train_params                  # Percentage Value for Training Set's Training Parameters\n",
    "            self.trainVal_params = 100 - settings.train_params              # Percentage Value for Training Set's Validation Parameters\n",
    "            self.testTrain_params = 100                                     # Percentage Value for Test Set's Training Parameters\n",
    "            self.testVal_params = 100 - self.testTrain_params               # Percentage Value for Test Set's Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        else:\n",
    "            assert(0 < settings.train_params <= self.num_params             # Numerical Limits for Number of Training Set's Parameters\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"\n",
    "            self.trainTrain_params = settings.train_params                  # Numerical Value for Training Set's Training Parameters\n",
    "            self.trainVal_params = self.num_params - settings.train_params  # Numerical Value for Training Set's Validation Parameters\n",
    "            self.testTrain_params = self.num_params                         # Numerical Value for Test Set's Training Parameters\n",
    "            self.testVal_params = self.num_params - self.testTrain_params   # Numerical Value for Test Set's Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Full MUDI Dataset Building\n",
    "        with alive_bar( self.num_patients,\n",
    "                        title = '1D MUDI Dataset',\n",
    "                        force_tty = True) as progress_bar:\n",
    "\n",
    "            # Training Set Scaffold Setting\n",
    "            #self.train_set = dict.fromkeys(('X_train', 'X_val', 'y_train', 'y_val'))\n",
    "            if self.voxel_wise:\n",
    "                X_train = np.empty(list(np.array((0, 1))))\n",
    "                X_val = np.empty(list(np.array((0, 1))))\n",
    "            else:\n",
    "                X_train = np.empty(list(np.array((0, settings.num_voxels))))\n",
    "                X_val = np.empty(list(np.array((0, settings.num_voxels))))\n",
    "            y_train = np.empty([0, self.num_labels]); y_val = np.empty([0, self.num_labels])\n",
    "\n",
    "            # Training Set Building / Training Patient Loop\n",
    "            for p in range(self.train_patients):\n",
    "\n",
    "                # Training Patient Data Access & Treatment\n",
    "                progress_bar.text = f\"\\n-> Training Set | Patient {self.patient_info['Patient'].iloc[p]}\"\n",
    "                pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                        train_params = self.trainTrain_params,\n",
    "                                                                        percentage = settings.percentage,\n",
    "                                                                        sample_shuffle = self.sample_shuffle)\n",
    "                X_train = np.concatenate((X_train, pX_train), axis = 0); X_val = np.concatenate((X_val, pX_val), axis = 0)\n",
    "                y_train = np.concatenate((y_train, py_train), axis = 0); y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "                time.sleep(0.01); progress_bar()\n",
    "            \n",
    "            # Training DataLoader Construction\n",
    "            #self.train_set['X_train'] = X_train; self.train_set['X_val'] = X_val\n",
    "            #self.train_set['y_train'] = y_train; self.train_set['y_val'] = y_val\n",
    "            self.trainTrainLoader = DataLoader(TensorDataset(   torch.Tensor(X_train),\n",
    "                                                                torch.Tensor(y_train)),\n",
    "                                                                num_workers = self.num_workers,\n",
    "                                                                batch_size = self.batch_size, shuffle = False)\n",
    "            self.trainValLoader = DataLoader(TensorDataset(     torch.Tensor(X_val),\n",
    "                                                                torch.Tensor(y_val)),\n",
    "                                                                num_workers = self.num_workers,\n",
    "                                                                batch_size = self.batch_size, shuffle = False)\n",
    "            del X_train, X_val, y_train, y_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "            # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Test Set Scaffold Setting\n",
    "            #self.test_set = dict.fromkeys(('X', 'y'))                       # Creation of Empty Dictionary to Fit Patient Data\n",
    "            if self.voxel_wise: X = np.empty(list(np.array((0, 1))))\n",
    "            else: X = np.empty(list(np.array((0, settings.num_voxels))))\n",
    "            y = np.empty([0, self.num_labels])\n",
    "\n",
    "            # Test Set Building / Test Patient Loop\n",
    "            for p in range(self.train_patients, self.train_patients + self.test_patients):\n",
    "\n",
    "                # Test Patient Data Access & Treatment\n",
    "                progress_bar.text = f\"-> Test Set | Patient {self.patient_info['Patient'].iloc[p]}\"\n",
    "                pX, py = self.split_patient(patient_number = p,\n",
    "                                            train_params = self.testTrain_params,\n",
    "                                            percentage = settings.percentage,\n",
    "                                            sample_shuffle = self.sample_shuffle)\n",
    "                X = np.concatenate((X, pX), axis = 0); y = np.concatenate((y, py), axis = 0)\n",
    "                print(X.shape)\n",
    "                time.sleep(0.01); progress_bar()\n",
    "\n",
    "            # Test DataLoader Construction\n",
    "            #self.test_set['X'] = X; self.test_set['y'] = y\n",
    "            self.testLoader = DataLoader(TensorDataset( torch.Tensor(X),\n",
    "                                                        torch.Tensor(y)),\n",
    "                                                        num_workers = self.num_workers,\n",
    "                                                        batch_size = self.batch_size, shuffle = False)\n",
    "            del X, y, pX, py\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Saving & Loading -------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Dataset Saving Function\n",
    "    def save(self):\n",
    "        if self.progress:\n",
    "\n",
    "            # Full Dataset Saving\n",
    "            f = open(f'{self.save_folderpath}/Horizontal 1D MUDI (Version {self.version})', 'wb')\n",
    "            pickle.dump(self, f); f.close\n",
    "\n",
    "            # Dataset Loader Saving\n",
    "            torch.save(self.trainTrainLoader, f\"{self.save_folderpath}/1D TrainTrainLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.trainValLoader, f\"{self.save_folderpath}/1D TrainValLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.testLoader, f\"{self.save_folderpath}/1D TestLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.scaler, f\"{self.save_folderpath}/1D Label Scaler (V{self.version}).pkl\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loading Function\n",
    "    def load(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Horizontal 1D MUDI (Version {version})', 'rb')\n",
    "        mudi = pickle.load(f)\n",
    "        f.close\n",
    "        return mudi\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loader Loading Function\n",
    "    def loader(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "        set_: str = 'Train',\n",
    "        mode_: str = 'Train',\n",
    "    ):\n",
    "        return torch.load(f\"{path}/1D {set_}{mode_}Loader (V{version}).pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal 1D MUDI Dataset Initialization Class\n",
    "class h1DMUDI(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "        \n",
    "        # Parameter Value Access\n",
    "        super(h1DMUDI).__init__()\n",
    "        self.settings = settings; self.version = self.settings.version\n",
    "        self.params = pd.read_excel(self.settings.param_filepath)       # List of Dataset's Parameters\n",
    "        self.num_params = self.params.shape[0]                          # Total Number of Parameters in Dataset\n",
    "        #assert(self.num_params == self.data.shape[0]), \"ERROR: Number of Parameters is Incoherent\" \n",
    "\n",
    "        # Patient Information Access\n",
    "        self.patient_info = pd.read_csv(self.settings.info_filepath)    # List of Patients and Corresponding IDs & Image Sizes inside Full Dataset\n",
    "        self.patient_info = self.patient_info[:-1]                      # Eliminating the Last Row containing Useless Information from the Patient Information\n",
    "        self.num_patients = self.patient_info.shape[0]                  # Number of Patients inside Full Dataset\n",
    "        self.progress = False                                           # Control Boolean Value for Progress Saving (Data can only be saved if Split)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Dataset Sample & Label Handling Settings\n",
    "        self.num_labels = self.settings.num_labels\n",
    "        if self.settings.gradient_coord: self.params = self.params.drop(columns = ['Gradient theta', 'Gradient phi'])   # Choosing of Gradient Orientation\n",
    "        else: self.params = self.params.drop(columns = ['Gradient x', 'Gradient y', 'Gradient z'])                      # from 3D Cartesian or Polar Referential\n",
    "        assert(self.params.shape[1] == self.num_labels), \"ERROR: Labels wrongly Deleted!\"\n",
    "        if self.settings.label_norm:                                                                                    # Control Boolean Value for the Normalization of Labels\n",
    "            self.scaler = StandardScaler()\n",
    "            self.params = pd.DataFrame( self.scaler.fit_transform(self.params),\n",
    "                                        columns = self.params.columns)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Patient & Parameter Shuffling Feature\n",
    "        if(self.settings.patient_shuffle): self.patient_info = self.patient_info.iloc[np.random.permutation(len(self.patient_info))]\n",
    "        self.trainStrat_filepath = Path(f\"{self.settings.save_folderpath}/1D Training Labels (V{self.version}).txt\")\n",
    "        self.valStrat_filepath = Path(f\"{self.settings.save_folderpath}/1D Validation Labels (V{self.version}).txt\")\n",
    "        if self.trainStrat_filepath.exists():\n",
    "            self.idx_train = np.loadtxt(self.trainStrat_filepath).astype(int)\n",
    "            if self.valStrat_filepath.exists(): self.idx_val = np.loadtxt(self.valStrat_filepath).astype(int)\n",
    "            else: self.idx_val = np.setdiff1d(np.array(range(0, self.num_params)), self.idx_train).astype(int)\n",
    "        else:\n",
    "            if self.settings.sample_shuffle: idx_y = np.random.permutation(self.num_params)\n",
    "            else: idx_y = range(0, self.num_params)\n",
    "            self.idx_train = idx_y[0 : self.settings.train_params].astype(int)\n",
    "            self.idx_val = idx_y[self.settings.train_params : self.num_params].astype(int)\n",
    "        self.y_train = self.params.iloc[self.idx_train]\n",
    "        self.y_val = self.params.iloc[self.idx_val]\n",
    "\n",
    "    ##############################################################################################\n",
    "    # --------------------------------- Feature & Label Handling ---------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Label Scaler Download & Reverse Transformation\n",
    "    def label_unscale(\n",
    "        path: Path,\n",
    "        version: int,\n",
    "        y: np.array or pd.DataFrame\n",
    "    ):\n",
    "\n",
    "        # Label Scaler Download & Reverse Usage\n",
    "        scaler = torch.load(f\"{path}/V{version}/1D Label Scaler (V{version}).pkl\")\n",
    "        return scaler.inverse_transform(y)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ---------------------------------- Data Access & Splitting ---------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Patient Data Access Function\n",
    "    def get_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "    ):\n",
    "\n",
    "        # Patient Data Information\n",
    "        assert(0 <= patient_number < self.num_patients), f\"ERROR: Input Patient not Found!\"         # Assertion for the Existence of the Requested Patient\n",
    "        patient_id = self.patient_info['Patient'].iloc[patient_number]                              # Patient ID contained within the Patient List\n",
    "        patient_filepath = Path(f\"{self.settings.patient_folderpath}/p{patient_id}.csv\")            # Patient Filepath from detailed Folder\n",
    "        mask_filepath = Path(f\"{self.settings.mask_folderpath}/p{patient_id}.nii\")                  # Mask Filepath from detailed Folder\n",
    "        \n",
    "        # Patient Data Access Memory Requirements\n",
    "        assert(patient_filepath.exists()                                                            # Assertion for the Existence of Patient File in said Folder\n",
    "        ), f\"Filepath for Patient {patient_id} is not in the Dataset!\"\n",
    "        assert(mask_filepath.exists()                                                               # Assertion for the Existence of Mask File in said Folder\n",
    "        ), f\"Filepath for Mask {patient_id} is not in the Dataset!\"\n",
    "        file_size = os.path.getsize(patient_filepath)                                               # Memory Space occupied by Patient File\n",
    "        mask_size = os.path.getsize(mask_filepath)                                                  # Memory Space occupied by Mask File\n",
    "        available_memory = psutil.virtual_memory().available                                        # Memory Space Available for Computation\n",
    "        assert(available_memory >= (file_size + mask_size)                                          # Assertion for the Existence of Available Memory Space\n",
    "        ), f\"ERROR: Dataset requires {file_size + mask_size}b, but only {available_memory}b is available!\"\n",
    "        \n",
    "        # Patient Data Access\n",
    "        pX = pd.read_csv(patient_filepath); del pX['Unnamed: 0']                                    # Full Patient Data\n",
    "        pMask = load_img(mask_filepath)                                                             # Patient Mask Data\n",
    "        #pX = unmask(pX, pMask); pX = pX.get_fdata()                                                # Unmasking of Full Patient Data\n",
    "        del available_memory, mask_size, file_size\n",
    "        return pX, pMask\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Patient Data Splitting Function\n",
    "    def split_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "        train_params: int = 500,            # Number / Percentage of Parameters to be used in the Training Section of the Patient\n",
    "    ):\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(self.settings.percentage):\n",
    "            assert(0 < train_params <= 100\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"       # Percentage Limits for Number of Training Parameters\n",
    "            train_params = train_params / 100                           # Percentage Value for Training Parameters\n",
    "            val_params = 1 - train_params                               # Percentage Value for Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Numerical Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"       # Numerical Limits for Number of Training Parameters\n",
    "            val_params = self.num_params - train_params                 # Numerical Value for Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Patient Dataset Splitting into Training & Validation Sets\n",
    "        pX, pMask = self.get_patient(patient_number)\n",
    "        py_train = self.y_train; py_val = self.y_val\n",
    "        pX_train = pX.iloc[self.idx_train]\n",
    "        pX_val = pX.iloc[self.idx_val]\n",
    "        del pX, pMask\n",
    "\n",
    "        # 1D Image to Voxel-Wise Sample Conversion\n",
    "        if self.settings.conversion:\n",
    "\n",
    "            # Inclusion of Patient ID Label\n",
    "            if self.settings.patient_id:\n",
    "                py_train['Patient'] = self.patient_info['Patient'].iloc[patient_number]\n",
    "                py_val['Patient'] = self.patient_info['Patient'].iloc[patient_number]\n",
    "            \n",
    "            # Feature & Label Conversion\n",
    "            #py_train = py_train.iloc[np.repeat(np.arange(len(label)), data.shape[1])]\n",
    "            #data = pd.DataFrame(data.to_numpy().reshape((data.shape[0] * data.shape[1], 1)))\n",
    "        return pX_train, pX_val, py_train, py_val\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Dataset Splitting Function\n",
    "    def split(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "\n",
    "        # Patient Number Variable Logging\n",
    "        assert(0 < self.settings.test_patients <= self.num_patients             # Limits for Number of Test Set Patients\n",
    "        ), f\"ERROR: Test Patient Number not Supported!\"\n",
    "        self.train_patients = self.num_patients - self.settings.test_patients   # Number of Patients to be used in the Training Set\n",
    "        self.test_patients = self.settings.test_patients                        # Number of Patients to be used in the Test Set\n",
    "        self.progress = True                                                    # Control Boolean Value for Progress Saving (Data can only be saved if Split)\n",
    "        \n",
    "        # Computation of Training & Validation Parameter Numbers\n",
    "        if(self.settings.percentage):\n",
    "            assert(0 < self.settings.train_params <= 100\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"     # Percentage Limits for Number of Training Set's Parameters\n",
    "            self.train_params = settings.train_params                       # Percentage Value for Training Set's Training Parameters\n",
    "            self.val_params = 100 - settings.train_params                   # Percentage Value for Training Set's Validation Parameters\n",
    "        else:\n",
    "            assert(0 < self.settings.train_params <= self.num_params\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"     # Numerical Limits for Number of Training Set's Parameters\n",
    "            self.train_params = settings.train_params                       # Numerical Value for Training Set's Training Parameters\n",
    "            self.val_params = self.num_params - self.settings.train_params  # Numerical Value for Training Set's Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Full MUDI Dataset Building\n",
    "        with alive_bar( self.num_patients,\n",
    "                        title = '1D MUDI Dataset',\n",
    "                        force_tty = True) as progress_bar:\n",
    "            \n",
    "            # Training & Test Sets Building\n",
    "            self.loaders = dict.fromkeys(('Train', 'Test'))\n",
    "            for m in ('Train', 'Test'):\n",
    "                if m == 'Train': patient_array = range(0, self.train_patients)\n",
    "                else: patient_array = range(self.train_patients, self.train_patients + self.test_patients)\n",
    "\n",
    "                # Set Scaffolds Initialization\n",
    "                if self.settings.conversion:\n",
    "                    X_train = np.empty(list(np.array((0, 1))))\n",
    "                    X_val = np.empty(list(np.array((0, 1))))\n",
    "                    y_train = np.empty(list(np.array((0, self.num_labels))))\n",
    "                    y_val = np.empty(list(np.array((0, self.num_labels))))\n",
    "                else:\n",
    "                    X_train = np.empty(list(np.array((0, self.train_params))))\n",
    "                    X_val = np.empty(list(np.array((0, self.val_params))))\n",
    "\n",
    "                # Set Patient Loop\n",
    "                for p in patient_array:\n",
    "\n",
    "                    # Training Patient Data Access & Treatment\n",
    "                    progress_bar.text = f\"\\n-> {m} Set | Patient {self.patient_info['Patient'].iloc[p]}\"\n",
    "                    pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                            train_params = self.train_params)\n",
    "                    X_train = np.concatenate((X_train, pX_train.T), axis = 0)\n",
    "                    X_val = np.concatenate((X_val, pX_val.T), axis = 0)\n",
    "                    if self.settings.conversion:\n",
    "                        y_train = np.concatenate((y_train, py_train), axis = 0)\n",
    "                        y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "                    time.sleep(0.01); progress_bar()\n",
    "            \n",
    "                    # Set DataLoader Construction\n",
    "                    if p == patient_array[-1]:\n",
    "                        if self.settings.conversion: self.loaders[m] =  DataLoader(TensorDataset(   torch.Tensor(X_train),\n",
    "                                                                                                    torch.Tensor(y_train),\n",
    "                                                                                                    torch.Tensor(X_val),\n",
    "                                                                                                    torch.Tensor(y_val)),\n",
    "                                                                                                    num_workers = self.settings.num_workers,\n",
    "                                                                                                    batch_size = self.settings.batch_size,\n",
    "                                                                                                    shuffle = False)\n",
    "                        else: self.loaders[m] =  DataLoader(TensorDataset(                          torch.Tensor(X_train),\n",
    "                                                                                                torch.Tensor(X_val)),\n",
    "                                                                                                num_workers = self.settings.num_workers,\n",
    "                                                                                                batch_size = self.settings.batch_size,\n",
    "                                                                                                shuffle = False)\n",
    "                        del X_train, X_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Saving & Loading -------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Dataset Saving Function\n",
    "    def save(self):\n",
    "        if self.progress:\n",
    "\n",
    "            # Full Dataset Saving\n",
    "            f = open(f'{self.settings.save_folderpath}/Horizontal 1D MUDI (Version {self.version})', 'wb')\n",
    "            pickle.dump(self, f); f.close\n",
    "\n",
    "            # Dataset Loader Saving\n",
    "            torch.save(self.loaders['Train'], f\"{self.settings.save_folderpath}/1D TrainLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.loaders['Test'], f\"{self.settings.save_folderpath}/1D TestLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.scaler, f\"{self.settings.save_folderpath}/1D Label Scaler (V{self.version}).pkl\")\n",
    "            np.savetxt(f\"{self.settings.save_folderpath}/1D Training Labels (V{self.version}).txt\", np.array(self.y_train))\n",
    "            np.savetxt(f\"{self.settings.save_folderpath}/1D Validation Labels (V{self.version}).txt\", np.array(self.y_val))\n",
    "            \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loading Function\n",
    "    def load(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Horizontal 1D MUDI (Version {version})', 'rb')\n",
    "        mudi = pickle.load(f); f.close\n",
    "        return mudi\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loader Loading Function\n",
    "    def loader(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "        mode_: str = 'Train',\n",
    "    ):\n",
    "        return torch.load(f\"{path}/1D {mode_}Loader (V{version}).pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal 1D MUDI Dataset Initialization Class\n",
    "class h1DMUDI(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "        \n",
    "        # Parameter Value Access\n",
    "        super(h1DMUDI).__init__()\n",
    "        self.settings = settings; self.version = self.settings.version\n",
    "        self.params = pd.read_excel(self.settings.param_filepath)       # List of Dataset's Parameters\n",
    "        self.num_params = self.params.shape[0]                          # Total Number of Parameters in Dataset\n",
    "        #assert(self.num_params == self.data.shape[0]), \"ERROR: Number of Parameters is Incoherent\" \n",
    "\n",
    "        # Patient Information Access\n",
    "        self.patient_info = pd.read_csv(self.settings.info_filepath)    # List of Patients and Corresponding IDs & Image Sizes inside Full Dataset\n",
    "        self.patient_info = self.patient_info[:-1]                      # Eliminating the Last Row containing Useless Information from the Patient Information\n",
    "        self.num_patients = self.patient_info.shape[0]                  # Number of Patients inside Full Dataset\n",
    "        self.progress = False                                           # Control Boolean Value for Progress Saving (Data can only be saved if Split)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Dataset Sample & Label Handling Settings\n",
    "        self.num_labels = self.settings.num_labels\n",
    "        if self.settings.gradient_coord: self.params = self.params.drop(columns = ['Gradient theta', 'Gradient phi'])   # Choosing of Gradient Orientation\n",
    "        else: self.params = self.params.drop(columns = ['Gradient x', 'Gradient y', 'Gradient z'])                      # from 3D Cartesian or Polar Referential\n",
    "        assert(self.params.shape[1] == self.num_labels), \"ERROR: Labels wrongly Deleted!\"\n",
    "        if self.settings.label_norm:                                                                                    # Control Boolean Value for the Normalization of Labels\n",
    "            self.scaler = StandardScaler()\n",
    "            self.params = pd.DataFrame( self.scaler.fit_transform(self.params),\n",
    "                                        columns = self.params.columns)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Patient & Parameter Shuffling Feature\n",
    "        if(self.settings.patient_shuffle): self.patient_info = self.patient_info.iloc[np.random.permutation(len(self.patient_info))]\n",
    "        self.trainStrat_filepath = Path(f\"{self.settings.save_folderpath}/1D Training Labels (V{self.version}).txt\")\n",
    "        self.valStrat_filepath = Path(f\"{self.settings.save_folderpath}/1D Validation Labels (V{self.version}).txt\")\n",
    "        if self.trainStrat_filepath.exists():\n",
    "            self.idx_train = np.loadtxt(self.trainStrat_filepath).astype(int)\n",
    "            if self.valStrat_filepath.exists(): self.idx_val = np.loadtxt(self.valStrat_filepath).astype(int)\n",
    "            else: self.idx_val = np.setdiff1d(np.array(range(0, self.num_params)), self.idx_train).astype(int)\n",
    "        else:\n",
    "            if self.settings.sample_shuffle: idx_y = np.random.permutation(self.num_params)\n",
    "            else: idx_y = range(0, self.num_params)\n",
    "            self.idx_train = idx_y[0 : self.settings.train_params].astype(int)\n",
    "            self.idx_val = idx_y[self.settings.train_params : self.num_params].astype(int)\n",
    "        self.y_train = self.params.iloc[self.idx_train]; self.y_val = self.params.iloc[self.idx_val]\n",
    "\n",
    "    ##############################################################################################\n",
    "    # --------------------------------- Feature & Label Handling ---------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Horizontal to Vertical Data Conversion Functonality\n",
    "    \"\"\"\n",
    "    def h2v_conversion(\n",
    "        self,\n",
    "\n",
    "    ):\n",
    "        \n",
    "        #\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Label Scaler Download & Reverse Transformation\n",
    "    def label_unscale(\n",
    "        path: Path,\n",
    "        version: int,\n",
    "        y: np.array or pd.DataFrame\n",
    "    ):\n",
    "\n",
    "        # Label Scaler Download & Reverse Usage\n",
    "        scaler = torch.load(f\"{path}/V{version}/1D Label Scaler (V{version}).pkl\")\n",
    "        return scaler.inverse_transform(y)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ---------------------------------- Data Access & Splitting ---------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Patient Data Access Function\n",
    "    def get_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "    ):\n",
    "\n",
    "        # Patient Data Information\n",
    "        assert(0 <= patient_number < self.num_patients), f\"ERROR: Input Patient not Found!\"         # Assertion for the Existence of the Requested Patient\n",
    "        patient_id = self.patient_info['Patient'].iloc[patient_number]                              # Patient ID contained within the Patient List\n",
    "        patient_filepath = Path(f\"{self.settings.patient_folderpath}/p{patient_id}.csv\")            # Patient Filepath from detailed Folder\n",
    "        mask_filepath = Path(f\"{self.settings.mask_folderpath}/p{patient_id}.nii\")                  # Mask Filepath from detailed Folder\n",
    "        \n",
    "        # Patient Data Access Memory Requirements\n",
    "        assert(patient_filepath.exists()                                                            # Assertion for the Existence of Patient File in said Folder\n",
    "        ), f\"Filepath for Patient {patient_id} is not in the Dataset!\"\n",
    "        assert(mask_filepath.exists()                                                               # Assertion for the Existence of Mask File in said Folder\n",
    "        ), f\"Filepath for Mask {patient_id} is not in the Dataset!\"\n",
    "        file_size = os.path.getsize(patient_filepath)                                               # Memory Space occupied by Patient File\n",
    "        mask_size = os.path.getsize(mask_filepath)                                                  # Memory Space occupied by Mask File\n",
    "        available_memory = psutil.virtual_memory().available                                        # Memory Space Available for Computation\n",
    "        assert(available_memory >= (file_size + mask_size)                                          # Assertion for the Existence of Available Memory Space\n",
    "        ), f\"ERROR: Dataset requires {file_size + mask_size}b, but only {available_memory}b is available!\"\n",
    "        \n",
    "        # Patient Data Access\n",
    "        pX = pd.read_csv(patient_filepath); del pX['Unnamed: 0']                                    # Full Patient Data\n",
    "        pMask = load_img(mask_filepath)                                                             # Patient Mask Data\n",
    "        #pX = unmask(pX, pMask); pX = pX.get_fdata()                                                # Unmasking of Full Patient Data\n",
    "        del available_memory, mask_size, file_size\n",
    "        return pX, pMask\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Patient Data Splitting Function\n",
    "    def split_patient(\n",
    "        self,\n",
    "        patient_number: int,                # Number for the Patient File being Read and Acquired (in Order)\n",
    "        train_params: int = 500,            # Number / Percentage of Parameters to be used in the Training Section of the Patient\n",
    "    ):\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Percentage Input)\n",
    "        if(self.settings.percentage):\n",
    "            assert(0 < train_params <= 100\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"       # Percentage Limits for Number of Training Parameters\n",
    "            train_params = train_params / 100                           # Percentage Value for Training Parameters\n",
    "            val_params = 1 - train_params                               # Percentage Value for Validation Parameters\n",
    "\n",
    "        # Computation of Training & Validation Parameter Numbers (Numerical Input)\n",
    "        else:\n",
    "            assert(0 < train_params <= self.num_params\n",
    "            ), f\"ERROR: Training Parameter Number not Supported!\"       # Numerical Limits for Number of Training Parameters\n",
    "            val_params = self.num_params - train_params                 # Numerical Value for Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Patient Dataset Splitting into Training & Validation Sets\n",
    "        pX, pMask = self.get_patient(patient_number)\n",
    "        py_train = self.y_train; py_val = self.y_val\n",
    "        pX_train = pX.iloc[self.idx_train]\n",
    "        pX_val = pX.iloc[self.idx_val]\n",
    "        del pX, pMask\n",
    "\n",
    "        # Inclusion of Patient ID Label\n",
    "        if self.settings.patient_id:\n",
    "            py_train['Patient'] = self.patient_info['Patient'].iloc[patient_number]\n",
    "            py_val['Patient'] = self.patient_info['Patient'].iloc[patient_number]\n",
    "\n",
    "        # 1D Image to Voxel-Wise Sample Conversion\n",
    "        if self.settings.conversion:\n",
    "            \n",
    "            # Feature & Label Conversion\n",
    "            py_train = pd.concat([py_train] * pX_train.shape[1], ignore_index = False)\n",
    "            py_val = pd.concat([py_val] * pX_val.shape[1], ignore_index = False)\n",
    "            pX_train = pd.DataFrame(np.ravel(pX_train), index = py_train.index)\n",
    "            pX_val = pd.DataFrame(np.ravel(pX_val), index = py_val.index)\n",
    "\n",
    "        return pX_train, pX_val, py_train, py_val\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Dataset Splitting Function\n",
    "    def split(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "\n",
    "        # Patient Number Variable Logging\n",
    "        assert(0 < self.settings.test_patients <= self.num_patients             # Limits for Number of Test Set Patients\n",
    "        ), f\"ERROR: Test Patient Number not Supported!\"\n",
    "        self.train_patients = self.num_patients - self.settings.test_patients   # Number of Patients to be used in the Training Set\n",
    "        self.test_patients = self.settings.test_patients                        # Number of Patients to be used in the Test Set\n",
    "        self.progress = True                                                    # Control Boolean Value for Progress Saving (Data can only be saved if Split)\n",
    "        \n",
    "        # Computation of Training & Validation Parameter Numbers\n",
    "        if(self.settings.percentage):\n",
    "            assert(0 < self.settings.train_params <= 100\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"         # Percentage Limits for Number of Training Set's Parameters\n",
    "            self.train_params = int(settings.train_params * self.num_params)    # Percentage Value for Training Set's Training Parameters\n",
    "        else:\n",
    "            assert(0 < self.settings.train_params <= self.num_params\n",
    "            ), f\"ERROR: Training Set's Parameter Number not Supported!\"         # Numerical Limits for Number of Training Set's Parameters\n",
    "            self.train_params = settings.train_params                           # Numerical Value for Training Set's Training Parameters\n",
    "        self.val_params = self.num_params - self.settings.train_params          # Numerical Value for Training Set's Validation Parameters\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Full MUDI Dataset Building\n",
    "        with alive_bar( self.num_patients,\n",
    "                        title = '1D MUDI Dataset',\n",
    "                        force_tty = True) as progress_bar:\n",
    "            \n",
    "            # Training & Test Sets Building\n",
    "            self.loaders = dict.fromkeys(('Train', 'Test'))\n",
    "            for m in ('Train', 'Test'):\n",
    "                if m == 'Train': patient_array = range(0, self.train_patients)\n",
    "                else: patient_array = range(self.train_patients, self.train_patients + self.test_patients)\n",
    "\n",
    "                # Set Scaffolds Initialization\n",
    "                if self.settings.conversion:\n",
    "                    X_train = np.empty(list(np.array((0, 1))))\n",
    "                    X_val = np.empty(list(np.array((0, 1))))\n",
    "                    y_train = np.empty(list(np.array((0, self.num_labels))))\n",
    "                    y_val = np.empty(list(np.array((0, self.num_labels))))\n",
    "                else:\n",
    "                    X_train = np.empty(list(np.array((0, self.train_params))))\n",
    "                    X_val = np.empty(list(np.array((0, self.val_params))))\n",
    "\n",
    "                # Set Patient Loop\n",
    "                for p in patient_array:\n",
    "\n",
    "                    # Training Patient Data Access & Treatment\n",
    "                    progress_bar.text = f\"\\n-> {m} Set | Patient {self.patient_info['Patient'].iloc[p]}\"\n",
    "                    pX_train, pX_val, py_train, py_val = self.split_patient(patient_number = p,\n",
    "                                                                            train_params = self.train_params)\n",
    "                    X_train = np.concatenate((X_train, pX_train.T), axis = 0)\n",
    "                    X_val = np.concatenate((X_val, pX_val.T), axis = 0)\n",
    "                    if self.settings.conversion:\n",
    "                        y_train = np.concatenate((y_train, py_train), axis = 0)\n",
    "                        y_val = np.concatenate((y_val, py_val), axis = 0)\n",
    "                    time.sleep(0.01); progress_bar()\n",
    "            \n",
    "                    # Set DataLoader Construction\n",
    "                    if p == patient_array[-1]:\n",
    "                        if self.settings.conversion: self.loaders[m] =  DataLoader(TensorDataset(   torch.Tensor(X_train),\n",
    "                                                                                                    torch.Tensor(y_train),\n",
    "                                                                                                    torch.Tensor(X_val),\n",
    "                                                                                                    torch.Tensor(y_val)),\n",
    "                                                                                                    num_workers = self.settings.num_workers,\n",
    "                                                                                                    batch_size = self.train_params,\n",
    "                                                                                                    shuffle = False)\n",
    "                        else: self.loaders[m] =  DataLoader(TensorDataset(                          torch.Tensor(X_train),\n",
    "                                                                                                    torch.Tensor(X_val)),\n",
    "                                                                                                    num_workers = self.settings.num_workers,\n",
    "                                                                                                    batch_size = self.settings.batch_size,\n",
    "                                                                                                    shuffle = False)\n",
    "                        del X_train, X_val, pX_train, pX_val, py_train, py_val\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Saving & Loading -------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Dataset Saving Function\n",
    "    def save(self):\n",
    "        if self.progress:\n",
    "\n",
    "            # Full Dataset Saving\n",
    "            f = open(f'{self.settings.save_folderpath}/Horizontal 1D MUDI (Version {self.version})', 'wb')\n",
    "            pickle.dump(self, f); f.close\n",
    "\n",
    "            # Dataset Loader Saving\n",
    "            torch.save(self.loaders['Train'], f\"{self.settings.save_folderpath}/1D TrainLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.loaders['Test'], f\"{self.settings.save_folderpath}/1D TestLoader (V{self.version}).pkl\")\n",
    "            torch.save(self.scaler, f\"{self.settings.save_folderpath}/1D Label Scaler (V{self.version}).pkl\")\n",
    "            np.savetxt(f\"{self.settings.save_folderpath}/1D Training Labels (V{self.version}).txt\", np.array(self.idx_train))\n",
    "            np.savetxt(f\"{self.settings.save_folderpath}/1D Validation Labels (V{self.version}).txt\", np.array(self.idx_val))\n",
    "            \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loading Function\n",
    "    def load(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "    ):\n",
    "        f = open(f'{path}/Horizontal 1D MUDI (Version {version})', 'rb')\n",
    "        mudi = pickle.load(f); f.close\n",
    "        return mudi\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Dataset Loader Loading Function\n",
    "    def loader(\n",
    "        path: Path,\n",
    "        version: int = 0,\n",
    "        set_: str = 'Train',\n",
    "        mode_: str = 'Train',\n",
    "    ):\n",
    "        return torch.load(f\"{path}/1D {set_}{mode_}Loader (V{version}).pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical 1D MUDI Dataset Initialization Class\n",
    "class v1DMUDI(Dataset):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser\n",
    "    ):\n",
    "        \n",
    "        # Parameter Value Access\n",
    "        super(h1DMUDI).__init__()\n",
    "        self.settings = settings\n",
    "        \n",
    "        # Original Data Access\n",
    "        with h5py.File(self.settings.data_filepath, 'r') as f:\n",
    "            self.data = f['data1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1344, 576946)\n",
      "(56, 92, 77)\n",
      "\n",
      "\n",
      "\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bcd3608460>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#with h5py.File(data_settings.data_filepath, 'r') as f:\n",
    "    #data = np.array(f['data1']).T\n",
    "print(data.shape)\n",
    "#img = unmask(data[0, 0:108300], pMask).get_fdata().T\n",
    "print(img.shape);print('\\n\\n')\n",
    "print(img)\n",
    "plt.imshow(img[25, :, :], cmap = plt.cm.binary)\n",
    "plt.imshow(pMask.slicer[25:26].get_fdata()[0], cmap = plt.cm.binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Reader Importing\n",
    "#sys.path.append(f\"{data_settings.main_folderpath}/Dataset Reader\")\n",
    "#from h1DMUDI import h1DMUDI\n",
    "\n",
    "# Dataset Version Creation\n",
    "data = h1DMUDI(data_settings)\n",
    "#pX, pMask = data.get_patient(0)\n",
    "#pX_train, pX_val, py_train, py_val = data.split_patient(0)\n",
    "data.split(data_settings)\n",
    "data.save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **VW-CVAE** *Model*\n",
    "\n",
    "### *Voxel-Wise Conditional Variational AutoEncoder*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model* **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# --------------------------------------- Encoder Build --------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# Encoder Model Class\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser    # Model Settings & Parametrizations\n",
    "        #num_labels: int = 5,                # Number of Labels contained in Dataset\n",
    "        #num_channel: int = 64,              # Number of Output Channels for Encoder\n",
    "        #num_layers: int = 3,                # Number of Main Convolutional Layers\n",
    "        #latent_dim: int = 64                # Latent Space Dimensionality\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(Encoder, self).__init__()\n",
    "        self.settings = settings\n",
    "\n",
    "        # Encoder Downsampling Architecture Definition\n",
    "        net = []; in_channel = 1 + settings.num_labels\n",
    "        for i in reversed(range(settings.num_layers)):\n",
    "            out_channel = int(settings.num_channel / (2 ** i))      # Current Main Layer's Output Channels\n",
    "            #k = 2 * (i + 1)                                        # Kernel Size Value (6 is too high for Voxel-Wise CVAE)\n",
    "            #print(f\"{in_channel} -> {out_channel}\")\n",
    "            net.append(nn.Sequential(                               # Main Layer Block Repeatable Architecture\n",
    "                nn.Conv1d(      in_channels = in_channel,\n",
    "                                out_channels = out_channel,\n",
    "                                kernel_size = 1, stride = 2, padding = 0),\n",
    "                nn.LeakyReLU(   inplace = True)))\n",
    "            in_channel = out_channel                                # Next Main Layer's Input Channels\n",
    "        self.net = nn.Sequential(*net)\n",
    "        \n",
    "        # Mean and LogVariance Computation Linear Layers\n",
    "        self.mean_layer = nn.Linear(    in_features = settings.num_channel,\n",
    "                                        out_features = settings.latent_dim)\n",
    "        self.logvar_layer = nn.Linear(  in_features = settings.num_channel,\n",
    "                                        out_features = settings.latent_dim)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Encoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor,      # 1D Image Input\n",
    "        y: np.ndarray or torch.Tensor       # Image Labels Input\n",
    "    ):\n",
    "\n",
    "        # Net Input Handling\n",
    "        X = torch.Tensor(X).view(-1, 1, 1).to(self.settings.device)                             # Input Features | [batch_size, 1,              1]\n",
    "        y = torch.Tensor(y).view(-1, self.settings.num_labels, 1).to(self.settings.device)      # Input Labels   | [batch_size, num_labels,     1]\n",
    "        input = torch.cat((X, y), dim = 1)                                                      # Encoder Output | [batch_size, 1+num_channel,  1]\n",
    "        \n",
    "        # Forward Propagation in Encoder Architecture\n",
    "        output = self.net(input)                                                                # Encoder Output | [batch_size, num_channel,    1]\n",
    "        z_mean = self.mean_layer(output.view(-1, self.settings.num_channel))                    # Latent Mean    | [batch_size, latent_dim]\n",
    "        z_logvar = self.logvar_layer(output.view(-1, self.settings.num_channel))                # Latent LogVar  | [batch_size, latent_dim]\n",
    "\n",
    "        # Display Settings for Experimental Model Version\n",
    "        if self.settings.model_version == 0:\n",
    "            print(f\"Encoder Input  | {list(input.shape)}\")\n",
    "            print(f\"Encoder Output | {list(output.shape)}\")\n",
    "            print(f\"Latent Mean    | {list(z_mean.shape)}\")\n",
    "            print(f\"Latent LogVar  | {list(z_logvar.shape)}\\n\")\n",
    "        return z_mean, z_logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# --------------------------------------- Decoder Build --------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# Decoder Model Class\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser    # Model Settings & Parametrizations\n",
    "        #num_labels: int = 5,                # Number of Labels contained in Dataset\n",
    "        #num_channel: int = 64,              # Number of Output Channels for Encoder\n",
    "        #num_layers: int = 3,                # Number of Main Convolutional Layers\n",
    "        #latent_dim: int = 64                # Latent Space Dimensionality\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(Decoder, self).__init__()\n",
    "        self.settings = settings\n",
    "\n",
    "        # Decoder Upsampling Architecture Definition\n",
    "        net = []; in_channel =  settings.num_channel\n",
    "        self.linear = nn.Linear(in_features = settings.latent_dim + settings.num_labels,\n",
    "                                out_features = settings.num_channel)\n",
    "        for i in range(settings.num_layers):\n",
    "            out_channel = int(settings.num_channel / (2 ** (i + 1)))    # Current Main Layer's Output Channels\n",
    "            if i == settings.num_layers - 1: out_channel = 1            # Last Layer's Single Voxel Output Channel\n",
    "            #k = 2 * (i + 1)                                            # Kernel Size Value (6 is too high for Voxel-Wise CVAE)\n",
    "            #print(f\"{in_channel} -> {out_channel}\")\n",
    "            net.append(nn.Sequential(                                   # Main Layer Block Repeatable Architecture\n",
    "                nn.ConvTranspose1d( in_channels = in_channel,\n",
    "                                    out_channels = out_channel,\n",
    "                                    kernel_size = 1, stride = 2, padding = 0),\n",
    "                nn.LeakyReLU(       inplace = True)))\n",
    "            in_channel = out_channel                                    # Next Main Layer's Input Channels\n",
    "        net.append(nn.Sigmoid()); self.net = nn.Sequential(*net)\n",
    "            \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Decoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        z: np.ndarray or torch.Tensor,      # Latent Dimension Representation\n",
    "        y: np.ndarray or torch.Tensor       # Image Labels Input\n",
    "    ):\n",
    "\n",
    "        # Net Input Handling\n",
    "        z = torch.Tensor(z).to(self.settings.device)        # Latent Representation | [batch_size, latent_dim]\n",
    "        y = torch.Tensor(y).to(self.settings.device)        # Input Labels          | [batch_size, num_labels]\n",
    "        input = torch.cat((z, y), dim = 1)                  # Decoder Input         | [batch_size, latent_dim + num_labels]\n",
    "\n",
    "        # Forward Propagation in Decoder Architecture\n",
    "        output = self.linear(input).view(-1, self.settings.num_channel, 1)\n",
    "        output = self.net(output).view(-1, 1)\n",
    "\n",
    "        # Display Settings for Experimental Model Version\n",
    "        if self.settings.model_version == 0:\n",
    "            print(f\"Decoder Input  | {list(input.shape)}\")\n",
    "            print(f\"Decoder Output | {list(output.shape)}\\n\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# ----------------------------------- Voxel-Wise CVAE Build ----------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# Decoder Model Class\n",
    "class VWCVAE(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser    # Model Settings & Parametrizations\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(VWCVAE, self).__init__()\n",
    "        self.settings = settings\n",
    "        self.encoder = Encoder(settings)      # Encoder Architecture Definition\n",
    "        self.decoder = Decoder(settings)      # Decoder Architecture Definition\n",
    "        \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Latent Dimension Reparametrization Gimmick Function\n",
    "    def reparametrize(\n",
    "        self,\n",
    "        z_mean: torch.Tensor,           # Latent Dimension Mean\n",
    "        z_logvar: torch.Tensor          # Latent Dimension Logarithmic Variance\n",
    "    ):\n",
    "\n",
    "        eps = torch.randn(  z_mean.size(0),                             # Epsilon Sampling from\n",
    "                            z_mean.size(1)).to(self.settings.device)    # Standard Normal Distribution\n",
    "        z = z_mean + (eps * torch.exp(z_logvar / 2.0))                  # Latent Dimension Representation\n",
    "        return z\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Encoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor,      # 1D Image Input\n",
    "        y: np.ndarray or torch.Tensor       # Image Labels Input\n",
    "    ):\n",
    "\n",
    "        # Forward Propagation in Encoder Architecture\n",
    "        X = torch.Tensor(X)                             # Input Features        | [batch_size, 1]\n",
    "        y = torch.Tensor(y).to(self.settings.device)    # Input Labels          | [batch_size, num_labels]\n",
    "        z_mean, z_logvar = self.encoder(X, y)           # Encoder Output        | [batch_size, latent_dim]\n",
    "        z = self.reparametrize(z_mean, z_logvar)        # Latent Representation | [batch_size, latent_dim]\n",
    "        output = self.decoder(z, y)                     # Decoder Output        | [batch_size, 1]\n",
    "\n",
    "        # Display Settings for Experimental Model Version\n",
    "        if self.settings.model_version == 0:\n",
    "            print(f\"Model Features Input  | {list(X.shape)}\")\n",
    "            print(f\"Model Labels Input    | {list(y.shape)}\")\n",
    "            print(f\"Latent Representation | {list(z.shape)}\")\n",
    "            print(f\"Model Output          | {list(output.shape)}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input  | [5000, 6, 1]\n",
      "Encoder Output | [5000, 64, 1]\n",
      "Latent Mean    | [5000, 128]\n",
      "Latent LogVar  | [5000, 128]\n",
      "\n",
      "Decoder Input  | [5000, 133]\n",
      "Decoder Output | [5000, 1]\n",
      "\n",
      "Model Features Input  | [5000, 1]\n",
      "Model Labels Input    | [5000, 5]\n",
      "Latent Representation | [5000, 128]\n",
      "Model Output          | [5000, 1]\n"
     ]
    }
   ],
   "source": [
    "# Data Initialization Simulation\n",
    "X = torch.rand((5000, 1))\n",
    "y = torch.ones((5000, 5))\n",
    "z = torch.rand((5000, 128))\n",
    "\n",
    "\"\"\"\n",
    "# Encoder Initialization & Usage Example\n",
    "enc = Encoder(model_settings)\n",
    "z_mean, z_logvar = enc(X, y)\n",
    "#summary(enc.net, (1 + model_settings.num_labels, 1))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Decoder Initialization & Usage Example\n",
    "dec = Decoder(model_settings)\n",
    "output = dec(z, y)\n",
    "#summary(dec.net, (model_settings.num_channel, 1))\n",
    "\"\"\"\n",
    "\n",
    "# Full Voxel-Wise CVAE Initialization & Usage Example\n",
    "model = VWCVAE(model_settings)\n",
    "output = model(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running** *Script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Wise CVAE Model Training, Validation & Testing Script Class\n",
    "class LitVWCVAE(pl.LightningModule):\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ----------------------------------- Model & Dataset Setup ----------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,              # Model Settings & Parametrizations\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super().__init__()\n",
    "        self.settings = settings\n",
    "        self.lr_decay_epochs = [80, 140]                # Epochs for Learning Rate Decay\n",
    "\n",
    "        # Model Initialization\n",
    "        self.model = VWCVAE(                self.settings)\n",
    "        self.optimizer = torch.optim.Adam(  self.model.parameters(),\n",
    "                                            lr = self.settings.base_lr,\n",
    "                                            weight_decay = self.settings.weight_decay, )\n",
    "        self.recon_criterion = nn.MSELoss(); self.past_epochs = 0\n",
    "\n",
    "        # Existing Model Checkpoint Loading\n",
    "        self.model_filepath = Path(f\"{self.settings.save_folderpath}/V{self.settings.model_version}/All4One (V{self.settings.model_version}).pth\")\n",
    "        if self.settings.model_version != 0 and self.model_filepath.exists():\n",
    "\n",
    "            # Checkpoint Fixing (due to the use of nn.DataParallel)\n",
    "            print(f\"DOWNLOADING All4One 2D VAE (Version {self.settings.model_version})\")\n",
    "            checkpoint = torch.load(self.model_filepath); self.checkpoint_fix = dict()\n",
    "            for sd, sd_value in checkpoint.items():\n",
    "                if sd == 'ModelSD' or sd == 'OptimizerSD':\n",
    "                    self.checkpoint_fix[sd] = OrderedDict()\n",
    "                    for key, value in checkpoint[sd].items():\n",
    "                        if key[0:7] == 'module.':\n",
    "                            self.checkpoint_fix[sd][key[7:]] = value\n",
    "                        else: self.checkpoint_fix[sd][key] = value\n",
    "                else: self.checkpoint_fix[sd] = sd_value\n",
    "            \n",
    "            # Application of Checkpoint's State Dictionary\n",
    "            self.model.load_state_dict(self.checkpoint_fix['ModelSD'])\n",
    "            self.optimizer.load_state_dict(self.checkpoint_fix['OptimizerSD'])\n",
    "            self.past_epochs = self.checkpoint_fix['Training Epochs']\n",
    "            torch.set_rng_state(self.checkpoint_fix['RNG State'])\n",
    "            del checkpoint\n",
    "        self.lr_schedule = torch.optim.lr_scheduler.ExponentialLR(  self.optimizer,     # Learning Rate Decay\n",
    "                                                    gamma = self.settings.lr_decay)     # in Chosen Epochs\n",
    "        self.model = nn.DataParallel(self.model.to(self.settings.device))\n",
    "\n",
    "    # Optimizer Initialization Function\n",
    "    def configure_optimizers(self): return super().configure_optimizers()\n",
    "\n",
    "    # Foward Functionality\n",
    "    def forward(self, X): return self.model(X)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Train Set DataLoader Download\n",
    "    def train_dataloader(self):\n",
    "        TrainTrainLoader = h1DMUDI.loader(  Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                            dim = 2, version = self.settings.data_version,\n",
    "                                            set_ = 'Test', mode_ = 'Train')\n",
    "                                            #set_ = 'Train', mode_ = 'Train')\n",
    "        self.train_batches = len(TrainTrainLoader)\n",
    "        return TrainTrainLoader\n",
    "    \n",
    "    # Validation Set DataLoader Download\n",
    "    def val_dataloader(self):\n",
    "        TrainValLoader = h1DMUDI.loader(Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        dim = 2, version = self.settings.data_version,\n",
    "                                        set_ = 'Test', mode_ = 'Train')\n",
    "                                        #set_ = 'Train', mode_ = 'Val')\n",
    "        self.val_batches = len(TrainValLoader)\n",
    "        return TrainValLoader\n",
    "\n",
    "    # Test Set DataLoader Download\n",
    "    def test_dataloader(self):\n",
    "        TestValLoader = h1DMUDI.loader( Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        dim = 2, version = self.settings.data_version,\n",
    "                                        set_ = 'Test', mode_ = 'Val')\n",
    "        self.test_batches = len(TestValLoader)\n",
    "        return TestValLoader\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Training Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_train_start(self):\n",
    "        \n",
    "        # Model Training Mode Setup\n",
    "        self.model.train()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # TensorBoard Logger Initialization\n",
    "        self.train_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Training Performance')\n",
    "\n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_loss = 0\n",
    "        self.train_kl_loss = 0\n",
    "        self.train_recon_loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Training Step / Batch Loop \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Data Handling\n",
    "        X_batch, ygt_batch = batch\n",
    "        X_batch = X_batch.type(torch.float).to(self.settings.device)\n",
    "        #ygt_batch = ygt_batch.type(torch.float).to(self.settings.device)\n",
    "\n",
    "        # Forward Propagation & Loss Computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **clNN** *Model*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model* **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# --------------------------------------- Encoder Class --------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# clNN Encoder Model Class\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,      # Model Settings & Parametrizations\n",
    "        #in_params: int = 500,                  # Number of Input Parameter Settings\n",
    "        #out_params: int = 1344,                # Number of Output Parameter Settings\n",
    "        #var_hidden: int = 128,                 # Deviance / Expansion of Hidden Layers\n",
    "        profiler = None                         # Pytorch Lightning Profiler\n",
    "    ) -> None:\n",
    "\n",
    "        # Class Variable & Buffer Logging\n",
    "        super(Encoder, self).__init__()\n",
    "        self.profiler = profiler or pl.profilers.PassThroughProfiler()\n",
    "        self.settings = settings\n",
    "        self.register_buffer('temp', torch.tensor(self.settings.max_temp))\n",
    "        self.register_buffer('max_temp', torch.tensor(self.settings.max_temp))\n",
    "        self.register_buffer('min_temp', torch.tensor(self.settings.min_temp))\n",
    "        self.register_buffer('reg_eps', torch.tensor(self.settings.reg_eps))\n",
    "        self.register_buffer('min_temp', torch.tensor(self.settings.min_temp))\n",
    "\n",
    "        # Random Weight Initialization\n",
    "        logits = nn.init.xavier_normal_(torch.empty(self.settings.in_params,\n",
    "                                                    self.settings.latent_dim))\n",
    "        self.logits = nn.Parameter(logits, requires_grad = True)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # clNN Encoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.Tensor         # 1D Voxel Array Input\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        with self.profiler.profile('Encoder'):\n",
    "            selection: torch.Tensor = None\n",
    "            \n",
    "            # Training Mode\n",
    "            if self.training:\n",
    "                uniform = torch.rand(self.logits.size()).to(X.device)\n",
    "                gumbel = -torch.log(-torch.log(uniform))\n",
    "                logits_noise = (self.logits + gumbel) / self.temp\n",
    "                sample = F.softmax(logits_noise, dim = 1); selection = sample\n",
    "            \n",
    "            # Validation Mode\n",
    "            else:\n",
    "                logits_argmax = torch.argmax(self.logits, len(self.logits.size()) - 1)\n",
    "                logits_discrete = F.one_hot(logits_argmax, num_classes = self.logits.size()[1])\n",
    "                selection = logits_discrete\n",
    "            \n",
    "        return torch.matmul(X, torch.transpose(selection.float(), 0, 1))\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Temperature Update Functionality\n",
    "    def update_temp(\n",
    "        self,\n",
    "        current_epoch: int,\n",
    "        max_epochs: int\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        self.temp = self.max_temp * torch.pow(\n",
    "            (self.min_temp / self.max_temp), (current_epoch / max_epochs))\n",
    "        return self.temp\n",
    "\n",
    "    # Logit Mean Computation Functionality\n",
    "    def calc_mean_max(self) -> torch.Tensor:\n",
    "\n",
    "        logits_softmax = F.softmax(self.logits, dim=1)\n",
    "        logits_max = torch.max(logits_softmax, 1).values\n",
    "        return torch.mean(logits_max)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Index Obtaining Functionality\n",
    "    def get_indexes(self):\n",
    "\n",
    "        val = torch.argmax(self.logits, 1)\n",
    "        return val\n",
    "\n",
    "    # Regularization Functionality\n",
    "    def regularization(self) -> float:\n",
    "        \n",
    "        selection = torch.clamp(F.softmax(self.logits, dim = 1), self.reg_eps, 1)\n",
    "        return torch.sum(F.relu(torch.norm(selection, 1, dim = 0) - self.reg_threshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# --------------------------------------- Decoder Class --------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# clNN Decoder Model Class\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,      # Model Settings & Parametrizations\n",
    "        #in_params: int = 500,                  # Number of Input Parameter Settings\n",
    "        #out_params: int = 1344,                # Number of Output Parameter Settings\n",
    "        #var_hidden: int = 128,                 # Deviance / Expansion of Hidden Layers\n",
    "        profiler = None                         # Pytorch Lightning Profiler\n",
    "    ) -> None:\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(Decoder, self).__init__()\n",
    "        self.settings = settings\n",
    "        \n",
    "        # Decoder Layer Size & Dimensionality\n",
    "        idx = np.arange(2 + self.settings.num_hidden)\n",
    "        data_idx = np.array([idx[0], idx[-1]])\n",
    "        data = np.array([self.settings.latent_dim, self.settings.out_params])\n",
    "        layer_dim = np.interp(idx, data_idx, data).astype(int)\n",
    "\n",
    "        # Decoder Architecture Construction\n",
    "        decoder = OrderedDict()\n",
    "        for i in range(1, len(layer_dim)):\n",
    "            if i == len(layer_dim) - 1:\n",
    "                decoder[f'Linear Layer #{i - 1}'] = nn.Linear(layer_dim[i - 1], layer_dim[i])\n",
    "            else:\n",
    "                decoder[f'Linear Layer #{i - 1}'] = nn.Linear(layer_dim[i - 1], layer_dim[i])\n",
    "                decoder[f'Leaky ReLU #{i - 1}'] = nn.LeakyReLU(self.settings.neg_slope)\n",
    "        self.decoder = nn.Sequential(decoder)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # clNN Encoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.Tensor         # 1D Voxel Array Input\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        return self.decoder(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x500 and 128x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(model_settings)\n\u001b[0;32m      7\u001b[0m decoder \u001b[39m=\u001b[39m Decoder(model_settings)\n\u001b[1;32m----> 8\u001b[0m h \u001b[39m=\u001b[39m encoder(X_real); X_fake \u001b[39m=\u001b[39m decoder(h)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [25], line 57\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     54\u001b[0m         logits_discrete \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(logits_argmax, num_classes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m])\n\u001b[0;32m     55\u001b[0m         selection \u001b[39m=\u001b[39m logits_discrete\n\u001b[1;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(X, torch\u001b[39m.\u001b[39;49mtranspose(selection\u001b[39m.\u001b[39;49mfloat(), \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x500 and 128x500)"
     ]
    }
   ],
   "source": [
    "# Data Initialization Example\n",
    "X_real = torch.rand((100, 500))\n",
    "y = torch.rand((100, 5))\n",
    "\n",
    "# Model Initialization & Usage Example\n",
    "encoder = Encoder(model_settings)\n",
    "decoder = Decoder(model_settings)\n",
    "h = encoder(X_real); X_fake = decoder(h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running** *Script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **fcNN** *Model*\n",
    "\n",
    "### *Previously Used Fully Connected Neural Network*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Personal** *Iteration*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model* **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Fully Connected Neural Network Model Class\n",
    "class fcNN(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_params: int = 500,                   # Number of Input Parameter Settings\n",
    "        out_params: int = 1344,                 # Number of Output Parameter Settings\n",
    "        num_hidden: int = 2                     # Number of NN Hidden Layers\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(fcNN, self).__init__()\n",
    "        self.in_params = in_params; self.out_params = out_params; self.num_hidden = num_hidden\n",
    "        assert(self.out_params > self.in_params\n",
    "               ),\"ERROR: Neural Network wrongly built!\"\n",
    "        net = []; num_neuron = self.in_params\n",
    "\n",
    "        # Neural Network Architecture Definition\n",
    "        num_fc = int(np.floor((self.out_params - self.in_params) / (self.num_hidden + 1)))\n",
    "        for i in range(self.num_hidden + 1):\n",
    "            if i == self.num_hidden: num_fc += 1\n",
    "            net.append( nn.Sequential(\n",
    "                            nn.Linear(num_neuron, num_neuron + num_fc),\n",
    "                            nn.LeakyReLU(inplace = True)))\n",
    "            num_neuron += num_fc\n",
    "        assert(num_neuron == self.out_params), \"ERROR: Neural Network wrongly built!\"\n",
    "        self.net = nn.Sequential(*net)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Neural Network Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor,      # 1D Image Input\n",
    "    ):\n",
    "    \n",
    "        # Forward Propagation in Neural Network Architecture\n",
    "        #X = torch.Tensor(X).to(self.settings.device)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([108300, 1344])\n"
     ]
    }
   ],
   "source": [
    "# Linear Fully Connected Neural Network Usage Example\n",
    "X = torch.rand((108300, 500))\n",
    "model = fcNN()\n",
    "out = model(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Running** *Script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcNN Model Training, Validation & Testing Script Class\n",
    "class LitfcNN(pl.LightningModule):\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ---------------------------------------- Model Setup ---------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,              # Model Settings & Parametrizations\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super().__init__()\n",
    "        self.settings = settings\n",
    "        self.lr_decay_epochs = [80, 140]                # Epochs for Learning Rate Decay\n",
    "\n",
    "        # Model Initialization\n",
    "        self.model = fcNN(                  in_params = self.settings.in_params,\n",
    "                                            out_params = self.settings.out_params,\n",
    "                                            num_hidden = self.settings.num_hidden)\n",
    "        self.optimizer = torch.optim.Adam(  self.model.parameters(),\n",
    "                                            lr = self.settings.base_lr,\n",
    "                                            weight_decay = self.settings.weight_decay)\n",
    "        self.criterion = nn.MSELoss(); self.recon_criterion = nn.MSELoss()\n",
    "        self.past_epochs = 0\n",
    "\n",
    "        # Existing Model Checkpoint Loading\n",
    "        self.model_filepath = Path(f\"{self.settings.save_folderpath}/V{self.settings.model_version}/fcNN (V{self.settings.model_version}).pth\")\n",
    "        if self.settings.model_version != 0 and self.model_filepath.exists():\n",
    "\n",
    "            # Checkpoint Fixing (due to the use of nn.DataParallel)\n",
    "            print(f\"DOWNLOADING Fully Connected Neural Network (Version {self.settings.model_version})\")\n",
    "            checkpoint = torch.load(self.model_filepath); self.checkpoint_fix = dict()\n",
    "            for sd, sd_value in checkpoint.items():\n",
    "                if sd == 'ModelSD' or sd == 'OptimizerSD':\n",
    "                    self.checkpoint_fix[sd] = OrderedDict()\n",
    "                    for key, value in checkpoint[sd].items():\n",
    "                        if key[0:7] == 'module.':\n",
    "                            self.checkpoint_fix[sd][key[7:]] = value\n",
    "                        else: self.checkpoint_fix[sd][key] = value\n",
    "                else: self.checkpoint_fix[sd] = sd_value\n",
    "            \n",
    "            # Application of Checkpoint's State Dictionary\n",
    "            self.model.load_state_dict(self.checkpoint_fix['ModelSD'])\n",
    "            self.optimizer.load_state_dict(self.checkpoint_fix['OptimizerSD'])\n",
    "            self.past_epochs = self.checkpoint_fix['Training Epochs']\n",
    "            torch.set_rng_state(self.checkpoint_fix['RNG State'])\n",
    "            del checkpoint\n",
    "        self.lr_schedule = torch.optim.lr_scheduler.ExponentialLR(  self.optimizer,     # Learning Rate Decay\n",
    "                                                    gamma = self.settings.lr_decay)     # in Chosen Epochs\n",
    "        self.model = nn.DataParallel(self.model.to(self.settings.device))\n",
    "        \n",
    "    # Optimizer Initialization Function\n",
    "    def configure_optimizers(self): return super().configure_optimizers()\n",
    "\n",
    "    # Foward Functionality\n",
    "    def forward(self, X): return self.model(X)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # -------------------------------------- Dataset Setup ---------------------------------------\n",
    "    ##############################################################################################\n",
    "    \n",
    "    # Train Set DataLoader Download\n",
    "    def train_dataloader(self):\n",
    "        TrainLoader = h1DMUDI.loader(  Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                            version = self.settings.data_version,\n",
    "                                            mode_ = 'Train')\n",
    "        self.train_batches = len(TrainLoader)\n",
    "        return TrainLoader\n",
    "\n",
    "    # Test Set DataLoader Download\n",
    "    def test_dataloader(self):\n",
    "        TestLoader = h1DMUDI.loader( Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        version = self.settings.data_version,\n",
    "                                        mode_ = 'Test')\n",
    "        self.test_batches = len(TestLoader)\n",
    "        return TestLoader\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Patient Image Reconstruction\n",
    "    def reconstruct(\n",
    "        self,\n",
    "        mode: str,\n",
    "        #sel_patient: int = 0,           # Selected Example Patient\n",
    "        sel_params: int = 0,            # Selected Parameter Setting Index\n",
    "        sel_slice: int = 25             # Selected Slice\n",
    "    ):\n",
    "        \n",
    "        # Dataset & Real 3D Image Access\n",
    "        num_epochs = self.past_epochs + self.current_epoch\n",
    "        if mode == 'Train':\n",
    "            if self.current_epoch == 0:\n",
    "                self.data = h1DMUDI.load(self.settings.data_folderpath, self.settings.data_version)\n",
    "                self.pX_train, self.pMask_train = self.data.get_patient(0)\n",
    "            pX = self.pX_train; pMask = self.pMask_train; sel_patient = 0\n",
    "        elif mode == 'Test':\n",
    "            if self.current_epoch == 0:\n",
    "                self.data = h1DMUDI.load(self.settings.data_folderpath, self.settings.data_version)\n",
    "                self.pX_test, self.pMask_test = self.data.get_patient(4)\n",
    "            pX = self.pX_test; pMask = self.pMask_test; sel_patient = 4\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Fake 3D Image Generation\n",
    "        pX_real = unmask(pX, pMask); pX_real = pX_real.get_fdata().T\n",
    "        pX_fake = self.model(torch.Tensor(np.array(pX)[self.data.idx_train.astype(int), :]).T)\n",
    "        pX_fake = unmask(pX_fake.detach().numpy().T, pMask); pX_fake = pX_fake.get_fdata().T\n",
    "        assert(np.all(pX_real.shape == pX_fake.shape)), \"ERROR: Unmasking went Wrong!\"\n",
    "        loss = self.recon_criterion(torch.Tensor(pX_fake), torch.Tensor(pX_real))\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Original Training Example Image Subplot\n",
    "        figure = plt.figure(num_epochs, figsize = (60, 60))\n",
    "        patient_id = self.data.patient_info['Patient'].iloc[sel_patient]\n",
    "        plt.tight_layout(); plt.title(f'Epoch #{num_epochs} | Patient #{patient_id}'\n",
    "                        + f' | Parameter Combo #{sel_params} | Slice #{sel_slice}')\n",
    "        plt.subplot(2, 1, 1, title = 'Original Image')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "        plt.imshow(pX_real[sel_params, sel_slice, :, :], cmap = plt.cm.binary)\n",
    "\n",
    "        # Reconstructed Training Example Image Subplot\n",
    "        plt.subplot(2, 1, 2, title = 'Reconstructed Image')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "        plt.imshow(pX_fake[sel_params, sel_slice, :, :], cmap = plt.cm.binary)\n",
    "        return figure, loss\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Training Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_train_start(self):\n",
    "        \n",
    "        # Model Training Mode Setup\n",
    "        self.model.train()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # TensorBoard Logger Initialization\n",
    "        self.train_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Training Performance')\n",
    "\n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_train_epoch_start(self): self.train_loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Training Step / Batch Loop \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Data Handling\n",
    "        X_train_batch, X_val_batch = batch\n",
    "        X_train_batch = X_train_batch.type(torch.float).to(self.settings.device)\n",
    "        X_batch = torch.cat((X_train_batch, X_val_batch), 1).type(torch.float).to(self.settings.device)\n",
    "\n",
    "        # Forward Propagation & Loss Computation\n",
    "        X_fake_batch = self.model(X_train_batch)\n",
    "        loss = self.criterion(X_fake_batch, X_batch)\n",
    "\n",
    "        # Backwards Propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del X_batch, X_train_batch, X_val_batch, X_fake_batch\n",
    "        return loss\n",
    "\n",
    "    # Functionality called upon the End of a Batch Training Step\n",
    "    def on_train_batch_end(self, loss, batch, batch_idx): self.train_loss = self.train_loss + loss['loss'].item()\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Functionality called upon the End of a Training Epoch\n",
    "    def on_train_epoch_end(self):\n",
    "\n",
    "        # Learning Rate Decay\n",
    "        if (self.trainer.current_epoch + 1) in self.lr_decay_epochs:\n",
    "            self.lr_schedule.step()\n",
    "\n",
    "        # Epoch Update for Losses & Reconstructed Images\n",
    "        num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.train_loss = self.train_loss / self.train_batches\n",
    "        train_plot, train_recon_loss = self.reconstruct(mode = 'Train',\n",
    "                                                        sel_params = 0,\n",
    "                                                        sel_slice = 25)\n",
    "        \n",
    "        # TensorBoard Logger Model Visualizer, Update for Scalar Values & Image Visualizer\n",
    "        if num_epochs == 0: self.train_logger.experiment.add_graph(self.model, torch.rand(1, self.settings.in_params))\n",
    "        self.train_logger.experiment.add_scalar(\"Loss\", self.train_loss, num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Reconstruction Loss\", train_recon_loss, num_epochs)\n",
    "        self.train_logger.experiment.add_figure(f'Image Reconstruction', train_plot, num_epochs)\n",
    "\n",
    "        # Model Checkpoint Saving\n",
    "        torch.save({'ModelSD': self.model.state_dict(),\n",
    "                    'OptimizerSD': self.optimizer.state_dict(),\n",
    "                    'Training Epochs': num_epochs,\n",
    "                    'RNG State': torch.get_rng_state()},\n",
    "                    self.model_filepath)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # -------------------------------------- Testing Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_test_start(self):\n",
    "\n",
    "        # TensorBoard Logger Initialization\n",
    "        self.model.eval()\n",
    "        self.test_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Testing Performance')\n",
    "    \n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_test_epoch_start(self): self.test_loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Test Step / Batch Loop \n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        # Data Handling\n",
    "        X_train_batch, X_val_batch = batch\n",
    "        X_train_batch = X_train_batch.type(torch.float).to(self.settings.device)\n",
    "        X_batch = torch.cat((X_train_batch, X_val_batch), 1).type(torch.float).to(self.settings.device)\n",
    "\n",
    "        # Forward Propagation & Loss Computation\n",
    "        X_fake_batch = self.model(X_train_batch)\n",
    "        loss = self.criterion(X_fake_batch, X_batch)\n",
    "        del X_batch, X_train_batch, X_val_batch, X_fake_batch\n",
    "        return loss\n",
    "\n",
    "    # Functionality called upon the End of a Batch Test Step\n",
    "    def on_test_batch_end(self, loss, batch, batch_idx): self.test_loss = self.test_loss + loss['loss'].item()\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Functionality called upon the End of a Training Epoch\n",
    "    def on_test_epoch_end(self):\n",
    "\n",
    "        # Epoch Update for Losses & Reconstructed Images\n",
    "        num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.test_loss = self.test_loss / self.test_batches\n",
    "        test_plot, test_recon_loss = self.reconstruct(  mode = 'Test',\n",
    "                                                        sel_params = 0,\n",
    "                                                        sel_slice = 25)\n",
    "        \n",
    "        # TensorBoard Logger Update for Scalar Values & Image Visualizer\n",
    "        self.test_logger.experiment.add_scalar(\"Loss\", self.test_loss, num_epochs)\n",
    "        self.test_logger.experiment.add_scalar(\"Reconstruction Loss\", test_recon_loss, num_epochs)\n",
    "        self.test_logger.experiment.add_figure(f'Image Reconstruction', test_plot, num_epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# ----------------------------------------- Dataset ------------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# Dataset Access\n",
    "sys.path.append(f\"{data_settings.main_folderpath}/Dataset Reader\")\n",
    "from h1DMUDI import h1DMUDI\n",
    "\n",
    "\"\"\"\n",
    "# Dataset Version Creation\n",
    "data = h1DMUDI(data_settings)\n",
    "data.split(data_settings)\n",
    "data.save()\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################################\n",
    "# --------------------------------------- fcNN Model -----------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# Full fcNN All4One 2D VAE Model Class Importing\n",
    "sys.path.append(model_settings.model_folderpath)\n",
    "from fcNN import fcNN\n",
    "sys.path.append(model_settings.script_folderpath)\n",
    "from LitfcNN import LitfcNN\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Model Initialization & Training\n",
    "fcNN = LitfcNN(model_settings)\n",
    "fcNN_trainer = pl.Trainer(      max_epochs = 3,\n",
    "                                #max_epochs = model_settings.num_epochs,\n",
    "                                devices = 1 if torch.cuda.is_available() else None,\n",
    "                                enable_progress_bar = True,\n",
    "                                callbacks = [pl.callbacks.TQDMProgressBar(refresh_rate = 1)])\n",
    "#fcNN_trainer.test(fcNN)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################################\n",
    "# ------------------------------------ Image Visualization -----------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# 3D Interactive Plotting Function\n",
    "def plot(\n",
    "    num_param: int = 0,\n",
    "    num_slice: int = 0\n",
    "):\n",
    "\n",
    "    # Original Training Example Image Subplot\n",
    "    figure = plt.figure(figsize = (15, 15))\n",
    "    patient_id = data.patient_info['Patient'].iloc[4]\n",
    "    plt.tight_layout(); plt.title(f'Test Patient #{patient_id} | Parameters #{num_param} | Slice #{num_slice}')\n",
    "    plt.subplot(1, 3, 1, title = 'Original Image')\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "    plt.imshow(X_real[num_param, num_slice, :, :], cmap = plt.cm.binary)\n",
    "\n",
    "    # Reconstructed Training Example Image & MSE Subplot\n",
    "    plt.subplot(1, 3, 2, title = 'Reconstructed Image')\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "    plt.imshow(X_fake[num_param, num_slice, :, :], cmap = plt.cm.binary)\n",
    "\n",
    "    # MSE Loss Heatmap Plotting\n",
    "    plt.subplot(1, 3, 3, title = 'MSE Loss Heatmap')\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "    plt.imshow(heatmap[0, num_slice, :, :], cmap = 'hot')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Test Patient Image Reconstruction\n",
    "try: data\n",
    "except NameError:\n",
    "\n",
    "    # Original Image Access & Fake Image Generation\n",
    "    data = h1DMUDI.load(model_settings.data_folderpath, model_settings.data_version)\n",
    "    X_real, X_mask = data.get_patient(4)\n",
    "    X_fake = fcNN.model(torch.Tensor(np.array(X_real)[data.idx_train.astype(int), :]).T)\n",
    "\n",
    "    # MSE Loss Heatmap Generation\n",
    "    heatmap = np.empty((1, X_real.shape[1]))\n",
    "    for p in range(X_real.shape[1]):\n",
    "        heatmap[0, p] = fcNN.criterion(X_fake.T[:, p], torch.Tensor(np.array(X_real.iloc[:, p])))\n",
    "\n",
    "    # Original & Fake Image Unmasking\n",
    "    X_real = unmask(X_real, X_mask); X_real = X_real.get_fdata().T\n",
    "    X_fake = unmask(X_fake.detach().numpy().T, X_mask); X_fake = X_fake.get_fdata().T\n",
    "    heatmap = unmask(heatmap, X_mask); heatmap = heatmap.get_fdata().T\n",
    "    assert(np.all(X_real.shape == X_fake.shape)), \"ERROR: Unmasking went Wrong!\"\n",
    "\n",
    "# Parameter + Slice Slider Interactive Construction\n",
    "param_slider = IntSlider(value = 0, min = 0, max = X_real.shape[0] - 1, description = 'Parameter', continuous_update = False)\n",
    "slice_slider = IntSlider(value = 0, min = 0, max = X_real.shape[1] - 1, description = 'Slice', continuous_update = False)\n",
    "interactive(plot, num_param = param_slider, num_slice = slice_slider)\n",
    "plot(500, 30)\n",
    "\n",
    "# Original & Fake Image Saving\n",
    "#img_real = nibabel.Nifti1Image(X_real.T, affine = np.eye(4)); img_real.header.get_xyzt_units()\n",
    "#img_fake = nibabel.Nifti1Image(X_fake.T, affine = np.eye(4)); img_fake.header.get_xyzt_units()\n",
    "#img_real.to_filename(Path(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Patient Image (fcNN V{model_settings.model_version}).nii.gz\"))\n",
    "#img_fake.to_filename(Path(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Patient Recon (fcNN V{model_settings.model_version}).nii.gz\"))\n",
    "#nibabel.save(img_real, Path(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Patient Image (fcNN V{model_settings.model_version}).nii.gz\"))\n",
    "#nibabel.save(img_fake, Path(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Patient Recon (fcNN V{model_settings.model_version}).nii.gz\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chantal's** *Iteration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################\n",
    "# ------------------------------------ Image Visualization -----------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# 3D Interactive Plotting Function\n",
    "def plot(\n",
    "    num_param: int = 0,\n",
    "    num_slice: int = 0\n",
    "):\n",
    "\n",
    "    # Original Training Example Image Subplot\n",
    "    figure = plt.figure(figsize = (15, 15))\n",
    "    #patient_id = data.patient_info['Patient'].iloc[4]\n",
    "    plt.tight_layout(); plt.title(f'Test Patient #{14} | Parameters #{num_param} | Slice #{num_slice}')\n",
    "    plt.subplot(1, 3, 1, title = 'Original Image')\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "    plt.imshow(X_real[num_param, num_slice, :, :], cmap = plt.cm.binary)\n",
    "\n",
    "    # Reconstructed Training Example Image & MSE Subplot\n",
    "    plt.subplot(1, 3, 2, title = 'Reconstructed Image')\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "    plt.imshow(X_fake[num_param, num_slice, :, :], cmap = plt.cm.binary)\n",
    "\n",
    "    # MSE Loss Heatmap Plotting\n",
    "    plt.subplot(1, 3, 3, title = 'MSE Loss Heatmap')\n",
    "    plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "    plt.imshow(heatmap[num_param, num_slice, :, :], cmap = 'hot')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Test Patient's Original & Fake Image Access\n",
    "try: X_real\n",
    "except NameError:\n",
    "    #X_real = nib.load(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Set Original.nii.gz\").get_fdata().T\n",
    "    X_real = np.real(nib.load(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Set Original.nii.gz\").dataobj).T\n",
    "    X_fake = np.real(nib.load(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Set Reconstruction.nii.gz\").dataobj).T\n",
    "    heatmap = np.real(nib.load(f\"{model_settings.save_folderpath}/V{model_settings.model_version}/Test Set MSE Heatmap.nii.gz\").dataobj).T\n",
    "\n",
    "# Parameter + Slice Slider Interactive Construction\n",
    "param_slider = IntSlider(value = 0, min = 0, max = X_real.shape[0] - 1, description = 'Parameter', continuous_update = False)\n",
    "slice_slider = IntSlider(value = 0, min = 0, max = X_real.shape[1] - 1, description = 'Slice', continuous_update = False)\n",
    "interactive(plot, num_param = param_slider, num_slice = slice_slider)\n",
    "plot(800, 25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **cglVNN** *Model*\n",
    "\n",
    "### *Conditional Generative Linear Voxel Neural Network*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model* **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Linear Neural Network Model Class\n",
    "class clNN(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int = 5,                    # Number of Input Parameter Settings\n",
    "        num_hidden: int = 2,                    # Number of NN Hidden Layers\n",
    "        var_hidden: int = 128                   # Deviance / Expansion of Hidden Layers\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(clNN, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.num_hidden = num_hidden\n",
    "        self.var_hidden = var_hidden\n",
    "        var_neuron = int(np.floor(self.var_hidden / self.num_hidden))\n",
    "\n",
    "        # Neural Network Architecture Definition\n",
    "        net = []; in_neuron = 1 + self.num_labels; out_neuron = var_neuron\n",
    "        for i in range(1, (2 * self.num_hidden) + 1):\n",
    "            if i == self.num_hidden: var_neuron = -var_neuron\n",
    "            elif i == 2 * self.num_hidden: out_neuron = 1\n",
    "            #print(f\"{in_neuron} -> {out_neuron}\")\n",
    "            net.append( nn.Sequential(\n",
    "                            nn.Linear(in_neuron, out_neuron),\n",
    "                            nn.LeakyReLU(inplace = True)))\n",
    "            in_neuron = out_neuron; out_neuron = out_neuron + var_neuron\n",
    "        self.net = nn.Sequential(*net)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Neural Network Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor,      # 1D Voxel Input\n",
    "        y: np.ndarray or torch.Tensor,      # 1D Image Label Input\n",
    "    ):\n",
    "    \n",
    "        # Forward Propagation in Neural Network Architecture\n",
    "        X = torch.Tensor(X)#.to(self.settings.device)\n",
    "        y = torch.Tensor(y)#.to(self.settings.device)\n",
    "        return self.net(torch.cat([X, y], dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Generative Linear Voxel Net Model Class\n",
    "class cglVNN(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int = 5,                    # Number of Input Parameter Settings\n",
    "        num_hidden: int = 2,                    # Number of NN Hidden Layers\n",
    "        var_hidden: int =64                     # Deviance / Expansion of Hidden Layers\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(cglVNN, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.num_hidden = num_hidden\n",
    "        self.var_hidden = var_hidden\n",
    "        var_neuron = int(np.floor(self.var_hidden / self.num_hidden))\n",
    "\n",
    "        # Neural Network Architecture Definition\n",
    "        net = []; in_neuron = 1 + (2 * self.num_labels); out_neuron = var_neuron\n",
    "        for i in range(1, (2 * self.num_hidden) + 1):\n",
    "            if i == self.num_hidden: var_neuron = -var_neuron\n",
    "            elif i == 2 * self.num_hidden: out_neuron = 1\n",
    "            #print(f\"{in_neuron} -> {out_neuron}\")\n",
    "            net.append( nn.Sequential(\n",
    "                            nn.Linear(in_neuron, out_neuron),\n",
    "                            nn.LeakyReLU(inplace = True)))\n",
    "            in_neuron = out_neuron; out_neuron = out_neuron + var_neuron\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Neural Network Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X_real: np.ndarray or torch.Tensor,     # 1D Voxel Input\n",
    "        y_real: np.ndarray or torch.Tensor,     # 1D Image Label Input\n",
    "        y_target: np.ndarray or torch.Tensor,   # 1D Target Label Input\n",
    "    ):\n",
    "    \n",
    "        # Forward Propagation in Neural Network Architecture\n",
    "        X_real = torch.Tensor(X_real)#.to(self.settings.device)\n",
    "        y_real = torch.Tensor(y_real)#.to(self.settings.device)\n",
    "        y_target = torch.Tensor(y_target)#.to(self.settings.device)\n",
    "        return self.net(torch.cat([X_real, y_real, y_target], dim = 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "# Linear Fully Connected Neural Network Usage Example\n",
    "X_train = torch.rand((100, 1))\n",
    "y_train = torch.rand((100, 5))\n",
    "y_val = torch.rand((100, 5))\n",
    "model = cglVNN()\n",
    "out = model(X_train, y_train, y_val)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running** *Script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cglVNN Model Training, Validation & Testing Script Class\n",
    "class LitcglVNN(pl.LightningModule):\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ---------------------------------------- Model Setup ---------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,              # Model Settings & Parametrizations\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super().__init__()\n",
    "        self.settings = settings\n",
    "        self.lr_decay_epochs = [80, 140]                # Epochs for Learning Rate Decay\n",
    "\n",
    "        # Model Initialization\n",
    "        self.model = cglVNN(                num_labels = self.settings.num_labels,\n",
    "                                            num_hidden = self.settings.num_hidden,\n",
    "                                            var_hidden = self.settings.var_hidden)\n",
    "        self.optimizer = torch.optim.Adam(  self.model.parameters(),\n",
    "                                            lr = self.settings.base_lr,\n",
    "                                            weight_decay = self.settings.weight_decay)\n",
    "        self.criterion = nn.MSELoss(); self.recon_criterion = nn.MSELoss()\n",
    "        self.past_epochs = 0\n",
    "\n",
    "        # Existing Model Checkpoint Loading\n",
    "        self.model_filepath = Path(f\"{self.settings.save_folderpath}/V{self.settings.model_version}/cglVNN (V{self.settings.model_version}).pth\")\n",
    "        if self.settings.model_version != 0 and self.model_filepath.exists():\n",
    "\n",
    "            # Checkpoint Fixing (due to the use of nn.DataParallel)\n",
    "            print(f\"DOWNLOADING Conditional Generative Linear Voxel Neural Network (Version {self.settings.model_version})\")\n",
    "            checkpoint = torch.load(self.model_filepath); self.checkpoint_fix = dict()\n",
    "            for sd, sd_value in checkpoint.items():\n",
    "                if sd == 'ModelSD' or sd == 'OptimizerSD':\n",
    "                    self.checkpoint_fix[sd] = OrderedDict()\n",
    "                    for key, value in checkpoint[sd].items():\n",
    "                        if key[0:7] == 'module.':\n",
    "                            self.checkpoint_fix[sd][key[7:]] = value\n",
    "                        else: self.checkpoint_fix[sd][key] = value\n",
    "                else: self.checkpoint_fix[sd] = sd_value\n",
    "            \n",
    "            # Application of Checkpoint's State Dictionary\n",
    "            self.model.load_state_dict(self.checkpoint_fix['ModelSD'])\n",
    "            self.optimizer.load_state_dict(self.checkpoint_fix['OptimizerSD'])\n",
    "            self.past_epochs = self.checkpoint_fix['Training Epochs']\n",
    "            torch.set_rng_state(self.checkpoint_fix['RNG State'])\n",
    "            del checkpoint\n",
    "        self.lr_schedule = torch.optim.lr_scheduler.ExponentialLR(  self.optimizer,     # Learning Rate Decay\n",
    "                                                    gamma = self.settings.lr_decay)     # in Chosen Epochs\n",
    "        self.model = nn.DataParallel(self.model.to(self.settings.device))\n",
    "        \n",
    "    # Optimizer Initialization Function\n",
    "    def configure_optimizers(self): return super().configure_optimizers()\n",
    "\n",
    "    # Foward Functionality\n",
    "    def forward(self, X_train, y_train, y_val): return self.model(X_train, y_train, y_val)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # -------------------------------------- Dataset Setup ---------------------------------------\n",
    "    ##############################################################################################\n",
    "    \n",
    "    # Train Set DataLoader Download\n",
    "    def train_dataloader(self):\n",
    "        TrainTrainLoader = h1DMUDI.loader(  Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                            version = self.settings.data_version,\n",
    "                                            set = 'Train', mode_ = 'Train')\n",
    "        TrainValLoader = h1DMUDI.loader(    Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                            version = self.settings.data_version,\n",
    "                                            set = 'Train', mode_ = 'Val')\n",
    "        assert(len(TrainTrainLoader) = len(TrainValLoader)\n",
    "               ), f\"ERROR: DataLoaders wrongly built!\"\n",
    "        self.train_batches = len(TrainTrainLoader)\n",
    "        return {'train': TrainTrainLoader, 'val': TrainValLoader}\n",
    "\n",
    "    # Test Set DataLoader Download\n",
    "    def test_dataloader(self):\n",
    "        TestLoader = h1DMUDI.loader( Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        version = self.settings.data_version,\n",
    "                                        mode_ = 'Test')\n",
    "        self.test_batches = len(TestLoader)\n",
    "        return TestLoader\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Patient Image Reconstruction\n",
    "    def reconstruct(\n",
    "        self,\n",
    "        sel_params: int = 0,            # Selected Parameter Setting Index\n",
    "        sel_slice: int = 25             # Selected Slice\n",
    "    ):\n",
    "        \n",
    "        # Fake 3D Image Generation\n",
    "        pX_real = unmask(self.pX, self.pMask); pX_real = pX_real.get_fdata().T\n",
    "        pX_fake = self.model(torch.Tensor(np.array(self.pX)[self.data.idx_train.astype(int), :]).T)\n",
    "        pX_fake = unmask(pX_fake.detach().numpy().T, self.pMask); pX_fake = pX_fake.get_fdata().T\n",
    "        assert(np.all(pX_real.shape == pX_fake.shape)), \"ERROR: Unmasking went Wrong!\"\n",
    "        loss = self.recon_criterion(torch.Tensor(pX_fake), torch.Tensor(pX_real))\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Original Training Example Image Subplot\n",
    "        figure = plt.figure(self.num_epochs, figsize = (60, 60))\n",
    "        patient_id = self.data.patient_info['Patient'].iloc[self.ex_patient]\n",
    "        plt.tight_layout(); plt.title(f'Epoch #{self.num_epochs} | Patient #{patient_id}'\n",
    "                        + f' | Parameter Combo #{sel_params} | Slice #{sel_slice}')\n",
    "        plt.subplot(2, 1, 1, title = 'Original Image')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "        plt.imshow(pX_real[sel_params, sel_slice, :, :], cmap = plt.cm.binary)\n",
    "\n",
    "        # Reconstructed Training Example Image Subplot\n",
    "        plt.subplot(2, 1, 2, title = 'Reconstructed Image')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "        plt.imshow(pX_fake[sel_params, sel_slice, :, :], cmap = plt.cm.binary)\n",
    "        return figure, loss\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Training Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_train_start(self):\n",
    "        \n",
    "        # Model Training Mode Setup\n",
    "        self.model.train()\n",
    "        self.automatic_optimization = False~\n",
    "        self.train_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Training Performance')\n",
    "\n",
    "        # Training & Validation Set Example Patient Dataset \n",
    "        self.data = h1DMUDI.load(self.settings.data_folderpath, self.settings.data_version)\n",
    "        self.ex_patient = 0; self.pX_train, self.pMask_train = self.data.split_patient(self.ex_patient)\n",
    "        self.pX = self.pX_train; self.pMask = self.pMask_train\n",
    "\n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_train_epoch_start(self):\n",
    "        self.num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.train_loss = 0\n",
    "        self.val_loss = 0\n",
    "        self.loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Training Step / Batch Loop \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Full Batch Input Data Handling\n",
    "        X_train_batch, y_train_batch = batch['train']\n",
    "        X_val_batch, y_val_batch = batch['val']\n",
    "        X_batch = torch.cat((X_train_batch, X_val_batch), dim = 0)\n",
    "        y_batch = torch.cat((y_train_batch, y_val_batch), dim = 0)\n",
    "        X_batch = X_batch.type(torch.float).to(self.settings.device)\n",
    "        y_batch = y_batch.type(torch.float).to(self.settings.device)\n",
    "\n",
    "        print(X_train_batch.shape); print(X_train_batch.shape)\n",
    "        print(X_val_batch.shape); print(y_val_batch.shape)\n",
    "        print(X_batch.shape); print(y_batch.shape)\n",
    "\n",
    "        # Voxel Reconstruction Loop (all Parameters)\n",
    "        assert(len(y_batch) == self.data.num_params * self.settings.num_labels\n",
    "               ), f\"ERROR: Batch Labels wrongly Set for Reconstruction!\"\n",
    "        tt_loss = torch.zeros(1); tv_loss = torch.zeros(1)\n",
    "        vt_loss = torch.zeros(1); vv_loss = torch.zeros(1)\n",
    "        for i in range(self.data.num_params):\n",
    "            \n",
    "            # Forward Propagation\n",
    "            y_target = y_batch[i, :].repeat(self.data.num_params, 1)\n",
    "            X_target = self.model(X_batch, y_batch, y_target)\n",
    "            print(X_target.shape); print(y_target.shape)\n",
    "\n",
    "            # Loss Computation\n",
    "            t_loss = self.criterion(X_target[0:self.data.train_params, :],\n",
    "                            X_batch[i, :].repeat(self.data.train_params, 1))\n",
    "            v_loss = self.criterion(X_target[self.data.train_params::, :],\n",
    "                            X_batch[i, :].repeat(self.data.val_params, 1))\n",
    "            if i < self.data.train_params:\n",
    "                tt_loss = tt_loss + (t_loss / self.data.train_params)\n",
    "                vt_loss = vt_loss + (v_loss / self.data.val_params)\n",
    "            else:\n",
    "                tv_loss = tv_loss + (t_loss / self.data.train_params)\n",
    "                vv_loss = vv_loss + (v_loss / self.data.val_params)\n",
    "            del X_target, y_target, t_loss, v_loss\n",
    "\n",
    "        # Backwards Propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        print(tt_loss); print(tv_loss); print(vt_loss); print(vv_loss)\n",
    "        tt_loss.backward(retain_graph = True)\n",
    "        tv_loss.backward(retain_graph = True)\n",
    "        self.optimizer.step()\n",
    "        del X_batch, X_train_batch, X_val_batch, X_train_batch, y_val_batch, y_batch\n",
    "        return {\"tt_loss\": tt_loss, \"vt_loss\": vt_loss, 'train_loss': tt_loss + tv_loss,\n",
    "                'tv_loss': tv_loss, 'vv_loss': vv_loss, 'val_loss': vt_loss + vv_loss,}\n",
    "\n",
    "    # Functionality called upon the End of a Batch Training Step\n",
    "    def on_train_batch_end(self, loss, batch, batch_idx):\n",
    "        self.tt_loss = self.tt_loss + loss['tt_loss'].item()\n",
    "        self.vt_loss = self.vt_loss + loss['vt_loss'].item()\n",
    "        self.tv_loss = self.tv_loss + loss['tv_loss'].item()\n",
    "        self.vv_loss = self.vv_loss + loss['vv_loss'].item()\n",
    "        self.train_loss = self.train_loss + loss['train_loss'].item()\n",
    "        self.val_loss = self.val_loss + loss['val_loss'].item()\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Functionality called upon the End of a Training Epoch\n",
    "    def on_train_epoch_end(self):\n",
    "\n",
    "        # Learning Rate Decay\n",
    "        if (self.trainer.current_epoch + 1) in self.lr_decay_epochs:\n",
    "            self.lr_schedule.step()\n",
    "\n",
    "        # Epoch Update for Losses & Reconstructed Images\n",
    "        self.tt_loss = self.tt_loss / self.train_batches\n",
    "        self.vt_loss = self.vt_loss / self.train_batches\n",
    "        self.tv_loss = self.tv_loss / self.train_batches\n",
    "        self.vv_loss = self.vv_loss / self.train_batches\n",
    "        self.train_loss = self.train_loss / self.train_batches\n",
    "        self.val_loss = self.val_loss / self.val_loss\n",
    "        \"\"\"\n",
    "        train_plot, recon_loss = self.reconstruct(  mode = 'Train',\n",
    "                                                    sel_params = 0,\n",
    "                                                    sel_slice = 25)\n",
    "        \"\"\"\n",
    "\n",
    "        # TensorBoard Logger Model Visualizer, Update for Scalar Values & Image Visualizer\n",
    "        if self.num_epochs == 0: self.train_logger.experiment.add_graph(self.model, torch.rand(1, self.settings.in_params))\n",
    "        self.train_logger.experiment.add_scalar(\"Train -> Train Reconstruction Loss\", self.tt_loss, self.num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Val -> Train Interpolation Loss\", self.vt_loss, self.num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Train -> Val Interpolation Loss\", self.tv_loss, self.num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Val -> Val Reconstruction Loss\", self.vv_loss, self.num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Total Training Loss\", self.train_loss, self.num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Total Validation Loss\", self.val_loss, self.num_epochs)\n",
    "        #self.train_logger.experiment.add_scalar(\"Image Reconstruction Loss\", recon_loss, self.num_epochs)\n",
    "        #self.train_logger.experiment.add_figure(f'Image Reconstruction', train_plot, self.num_epochs)\n",
    "\n",
    "        # Model Checkpoint Saving\n",
    "        torch.save({'ModelSD': self.model.state_dict(),\n",
    "                    'OptimizerSD': self.optimizer.state_dict(),\n",
    "                    'Training Epochs': self.num_epochs,\n",
    "                    'RNG State': torch.get_rng_state()},\n",
    "                    self.model_filepath)\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # -------------------------------------- Testing Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_test_start(self):\n",
    "\n",
    "        # TensorBoard Logger Initialization\n",
    "        self.model.eval()\n",
    "        self.test_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Testing Performance')\n",
    "\n",
    "        # Test Set Example Patient Dataset Access\n",
    "        self.data = h1DMUDI.load(self.settings.data_folderpath, self.settings.data_version)\n",
    "        ex_patient = 4; self.pX_train, self.pMask_train = self.data.get_patient(ex_patient)\n",
    "        self.pX = self.pX_test; self.pMask = self.pMask_test\n",
    "    \n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_test_epoch_start(self):\n",
    "        self.num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.test_loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Test Step / Batch Loop \n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        # Data Handling\n",
    "        X_train_batch, X_val_batch = batch\n",
    "        X_train_batch = X_train_batch.type(torch.float).to(self.settings.device)\n",
    "        X_batch = torch.cat((X_train_batch, X_val_batch), 1).type(torch.float).to(self.settings.device)\n",
    "\n",
    "        # Forward Propagation & Loss Computation\n",
    "        X_fake_batch = self.model(X_train_batch)\n",
    "        loss = self.criterion(X_fake_batch, X_batch)\n",
    "        del X_batch, X_train_batch, X_val_batch, X_fake_batch\n",
    "        return loss\n",
    "\n",
    "    # Functionality called upon the End of a Batch Test Step\n",
    "    def on_test_batch_end(self, loss, batch, batch_idx): self.test_loss = self.test_loss + loss['loss'].item()\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Functionality called upon the End of a Training Epoch\n",
    "    def on_test_epoch_end(self):\n",
    "\n",
    "        # Epoch Update for Losses & Reconstructed Images\n",
    "        num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.test_loss = self.test_loss / self.test_batches\n",
    "        test_plot, test_recon_loss = self.reconstruct(  mode = 'Test',\n",
    "                                                        sel_params = 0,\n",
    "                                                        sel_slice = 25)\n",
    "        \n",
    "        # TensorBoard Logger Update for Scalar Values & Image Visualizer\n",
    "        self.test_logger.experiment.add_scalar(\"Loss\", self.test_loss, num_epochs)\n",
    "        self.test_logger.experiment.add_scalar(\"Reconstruction Loss\", test_recon_loss, num_epochs)\n",
    "        self.test_logger.experiment.add_figure(f'Image Reconstruction', test_plot, num_epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Access\n",
    "sys.path.append(f\"{data_settings.main_folderpath}/Dataset Reader\")\n",
    "from h1DMUDI import h1DMUDI\n",
    "data = h1DMUDI.load(model_settings.data_folderpath, model_settings.data_version)\n",
    "ex_patient = 0; pX, pMask = data.get_patient(ex_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = dict.fromkeys(('Train', 'Test'))\n",
    "train_X = np.random.rand(500 * 3, 1); train_y = np.random.rand(500 * 3, 5)\n",
    "val_X = np.random.rand(844 * 3, 1); val_y = np.random.rand(844 * 3, 5)\n",
    "loaders['TrainTrain'] =  DataLoader(TensorDataset(  torch.Tensor(train_X),\n",
    "                                                    torch.Tensor(train_y)), \n",
    "                                                    num_workers = 20,\n",
    "                                                    batch_size = 500,\n",
    "                                                    shuffle = False)\n",
    "loaders['TrainVal'] =   DataLoader(TensorDataset(   torch.Tensor(val_X),\n",
    "                                                    torch.Tensor(val_y)),  \n",
    "                                                    num_workers = 20,\n",
    "                                                    batch_size = 844,\n",
    "                                                    shuffle = False)\n",
    "torch.save(loaders['TrainTrain'], f\"{data_settings.save_folderpath}/1D {'TrainTrain'}Loader (V{1}).pkl\")\n",
    "torch.save(loaders['TrainVal'], f\"{data_settings.save_folderpath}/1D {'TrainVal'}Loader (V{1}).pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **CSeq2Seq NN** *Model*\n",
    "\n",
    "### *Conditional Sequence-2-Sequence Neural Network*\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model* **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parameters Setting\n",
    "in_channel = 100\n",
    "latent_dim = 128\n",
    "num_layers = 3\n",
    "\n",
    "# Data Initialization Simulation\n",
    "X = torch.rand((100, in_channel))\n",
    "y = torch.ones((100, 5))\n",
    "z = torch.rand((100, 128))\n",
    "\n",
    "# Encoder Initialization & Usage Example\n",
    "encoder = nn.LSTM(in_channel, latent_dim, num_layers, batch_first=True)\n",
    "_, (context, cell) = encoder(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7e2413ca9464f5b18ee008ec75e3890212b75ca17b4a3699f34f03bf3acaeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
