{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Initial* **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Library** *Settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Download\n",
    "%pip install --upgrade pip\n",
    "%pip install numpy\n",
    "%pip install argparse\n",
    "%pip install pandas\n",
    "%pip install scipy\n",
    "%pip install torch\n",
    "%pip install sklearn\n",
    "%pip install keras==2.10\n",
    "%pip install tensorflow\n",
    "%pip install matplot\n",
    "%pip install plotly\n",
    "%pip install h5py\n",
    "%pip install h5pyViewer\n",
    "%pip install pytorch_lightning\n",
    "%pip install dipy\n",
    "%pip install openpyxl\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution(config=None, device_policy=None, execution_mode=None)\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import h5py\n",
    "import dipy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Functionality Import\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Callable, Dict, Literal, Optional, Union, Tuple\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_lightning.utilities.cli import DATAMODULE_REGISTRY\n",
    "from tensorflow.keras.layers import concatenate, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf._api.v2.compat.v1.disable_v2_behavior()\n",
    "from dipy.reconst.shm import cart2sphere, real_sh_descoteaux_from_index, sph_harm_ind_list\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Control** *Station*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser Initialization\n",
    "parser = argparse.ArgumentParser(\n",
    "    description = \"MUDIVisualizer\")\n",
    "\n",
    "# Filepath Arguments\n",
    "path = parser.add_argument_group('Required Filepaths')\n",
    "main_folderpath = '../../../Datasets/MUDI Dataset/'\n",
    "path.add_argument('--param_filepath', type = Path, default = main_folderpath + 'Raw Data/parameters_new.xlsx',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Parameters')\n",
    "path.add_argument('--patient_folderpath', type = Path, default = main_folderpath + 'Patient Data',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Patient Information')\n",
    "path.add_argument('--info_filepath', type = Path, default = main_folderpath + 'Raw Data/header1_.csv',\n",
    "                    help = 'Filepath for DHDF5 File containing MUDI Dataset Parameters')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Control Arguments for Datasets' Organization\n",
    "data = parser.add_argument_group(\"MUDI Dataset's Control Parameters\")\n",
    "data.add_argument('--batch_size', type = int, default = 500,\n",
    "                    help = \"Batch Size for DataLoaders\")\n",
    "data.add_argument('--vShuffle', type = bool, default = False,\n",
    "                    help = 'Control Variable for Vertical / Voxel Shuffle in Dataset')\n",
    "data.add_argument('--hShuffle', type = bool, default = False,\n",
    "                    help = 'Control Variable for Horizontal / Parameter Shuffle in Dataset')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Control Arguments for CVAE Model's Parameters\n",
    "cvae = parser.add_argument_group(\"CVAE Control Parameters\")\n",
    "cvae.add_argument('--nEpoch', type = int, default = 50,\n",
    "                    help = \"Number of Epochs for Model Training\")\n",
    "cvae.add_argument('--latentK', type = int, default = 50,\n",
    "                    help = \"Latent Space Dimensionality\")\n",
    "cvae.add_argument('--alpha', type = float, default = 0.001,\n",
    "                    help = \"Optimizers' Learning Rate Value\")\n",
    "\n",
    "# CVAE Version II - Keras Arguments\n",
    "cvae_keras = cvae.add_argument_group(\"Keras V2\")\n",
    "cvae_keras.add_argument('--nnDim', type = int, default = 512,\n",
    "                        help = \"Encoder's Neural Network Dimensionality\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "parse = parser.parse_args(\"\")\n",
    "parse.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data** *Access*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Access Requirements\n",
    "sys.path.append('../../../Datasets/MUDI Dataset/Dataset Reader')\n",
    "from v1DMUDI import v1DMUDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Patient 11's Data to the Training Set...\n",
      "Adding Patient 12's Data to the Training Set...\n",
      "Adding Patient 13's Data to the Training Set...\n",
      "Adding Patient 14's Data to the Training Set...\n",
      "Adding Patient 15's Data to the Test Set...\n",
      "╒══════════════╤════════════════╤═══════════════════════╤═════════════════════════╕\n",
      "│              │   No. Patients │ Training Parameters   │ Validation Parameters   │\n",
      "╞══════════════╪════════════════╪═══════════════════════╪═════════════════════════╡\n",
      "│ Training Set │              4 │ 500 (37.2%)           │ 844 (62.8%)             │\n",
      "├──────────────┼────────────────┼───────────────────────┼─────────────────────────┤\n",
      "│ Test Set     │              1 │ 20 (0.01%)            │ 1324 (0.99%)            │\n",
      "╘══════════════╧════════════════╧═══════════════════════╧═════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Dataset Initialization & Saving Example\n",
    "mudi = v1DMUDI(   parse.patient_folderpath,\n",
    "                parse.param_filepath,\n",
    "                parse.info_filepath)\n",
    "mudi.split()                                                # Dataset Splitting\n",
    "mudi.save(Path(f\"{main_folderpath}Saved Data\"))             # Dataset Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "mudi = vMUDI.load(Path(f\"{main_folderpath}Saved Data\"), version = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Pre-Processing* **Convolutional Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Image Pre-Processing Function\n",
    "    def pre_process(\n",
    "        self,\n",
    "        img: pd.DataFrame,\n",
    "    ):\n",
    "\n",
    "        # Input Variable Assertions\n",
    "        assert(img.ndim == 2), \"ERROR: Input Image Shape not Supported! (2D Arrays only)\"\n",
    "        assert(self.pre_shape < img.shape[1]), \"ERROR: Convolution Layer Size must be smaller than Original Image's no. of Voxels!\"\n",
    "\n",
    "        # Convolutional ayer Creation (using Patient's No. Voxels)\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(   self.pre_shape, (1, 1),\n",
    "                            padding = 'same',\n",
    "                            activation = 'relu',\n",
    "                            input_shape = (1, self.num_params, img.shape[1])))\n",
    "\n",
    "        # Input Formatting & Pre-Processing\n",
    "        self.img = img\n",
    "        img = tf.convert_to_tensor(img.values, dtype = 'float32')           # Pandas DataFrame -> Tensorflow Tensor\n",
    "        img = tf.reshape(img, (1, 1, self.num_params, img.shape[1]))        # Convolutional Input Tensor Reshapping 2D -> 4D\n",
    "        img = tf.reshape(model(img), (self.num_params, self.pre_shape))     # Convolutional Output Tensor Reshapping 4D -> 2D\n",
    "        self.img_final = img\n",
    "        return img.eval(session=tf.compat.v1.Session())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "a.eval(session=tf.compat.v1.Session())  \n",
    "#a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class preProcess(nn.Module):\n",
    "    \n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_shape: int = 108300,\n",
    "        final_shape: int = 100000,\n",
    "    ):\n",
    "\n",
    "        # Neural Convolution Building\n",
    "        super(preProcess, self).__init__()\n",
    "        self.final_shape = final_shape\n",
    "        self.conv1 = nn.Conv2d(img_shape, self.final_shape, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.linear = nn.Dens\n",
    "    \n",
    "    #\n",
    "    def forward(self, img_df: pd.DataFrame):\n",
    "\n",
    "        # Input Variable Assertions\n",
    "        assert(img_df.ndim == 2), \"ERROR: Input Image Shape not Supported! (2D Arrays only)\"\n",
    "        assert(self.final_shape < img_df.shape[1]), \"ERROR: Convolution Layer Size must be smaller than Original Image's no. of Voxels!\"\n",
    "\n",
    "        # Neural Network Application\n",
    "        img_df = torch.tensor(img_df.values.astype(np.float32))\n",
    "        for i in range(img_df.shape[0]):\n",
    "            img = np.array(img_df[i]).reshape((img_df.shape[1], 1, 1, 1))\n",
    "            img_df[i] = self.conv1(img)\n",
    "        return img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = preProcess()\n",
    "out = model(mudi.img)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Pre-Processing* **Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(mudi.img)\n",
    "cumulative_sum = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(np.argmax(cumulative_sum >= 0.99) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model** Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Keras** *Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since input values will have a *different number of voxels from patient to patient*, it is a requirement that the Encoder can have different input sizes, hence the use of **Global Average Pooling** for each of the batches. The first try was using 2D Pooling, and the intuition would say that this does work, since there is no need to have the encoder's input shape change from batch to batch, only when changing patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2512756107.py, line 133)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [26], line 133\u001b[1;36m\u001b[0m\n\u001b[1;33m    callbacks = [EarlyStopping(patience = 5)])                                                  # Inclusion of Early Stopping\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Keras CVAE Model Implementation Class\n",
    "class Keras_CVAE():\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        # Dataset Handling Requirements\n",
    "        data: vMUDI,                                # Dataset containing Parameters, Training and Test Set, and Everything inbetween\n",
    "        pre_shape: int = 1200,                    # Intermediate Dataset Shape as of Pre-Processing\n",
    "        #\n",
    "        \n",
    "        # Model Creation & Optimization Requirements\n",
    "        model_folderpath: Path = Path(\"\"),          # Path for Model Saving Folder\n",
    "        activ: str = 'relu',                        # Activation Function (Default: ReLU)\n",
    "        conv_nn: int = 512,                         # Encoder Convolutional Neural Network Layer Shape\n",
    "        lr: float = 0.001,                          # Learning Rate for Training Mode of both Datasets\n",
    "        latent_k: int = 50,                         # Latent Space Dimensionality\n",
    "        display: bool = False,                      # Control Variable for the Display of Error Values, etc\n",
    "    ):\n",
    "\n",
    "        # Class Requirement Variable Assertions\n",
    "        #assert\n",
    "\n",
    "        # Class Requirement Variable Logging\n",
    "        super(Keras_CVAE).__init__()\n",
    "        self.data = data; self.pre_shape = pre_shape\n",
    "        self.model_folderpath = model_folderpath\n",
    "        self.activ = activ; self.conv_nn = conv_nn\n",
    "        self.lr = lr; self.latent_k = latent_k\n",
    "        self.display = display; self.arch()\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Model Architecture Building Function\n",
    "    def arch(self,):\n",
    "\n",
    "        print(\"Building CVAE Model...\")\n",
    "\n",
    "        # Data Dimensionality Definition\n",
    "        X = Input(shape = (self.pre_shape, ))                           # Image Data Shape ---- [Batch Size (??), Intermediate Pre-Processed Size]\n",
    "        y = Input(shape = ((self.data.params.shape[1], )))              # Image Label Shape --- [?, 7]\n",
    "        input = concatenate([X, y], axis = 1)                           # Encoder Input Concatenation\n",
    "        if self.display: print(f\"    Encoder Input: {input.shape} [X: {X.shape} + y: {y.shape}]\")\n",
    "\n",
    "        # Encoder Architecture & Pipeline\n",
    "        optim = Adam(lr = self.lr)                                      # Encoder Optimizer Function\n",
    "        encoder = Dense(self.conv_nn, activation = self.activ)(input)   # Encoder Layer Definition\n",
    "        mu = Dense(self.latent_k, activation  = 'linear')(encoder)      #\n",
    "        sigma = Dense(self.latent_k, activation = 'linear')(encoder)    #\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # Latent Space Sampling Function\n",
    "        def sample(args):\n",
    "            mu, sigma = args\n",
    "            eps = K.random_normal(  shape = (self.latent_k, ), \n",
    "                                    mean = 0.0, stddev = 1.0)\n",
    "            return mu + K.exp(sigma / 2) * eps\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # Latent Space \n",
    "        z = Lambda( sample,                                              # Encoder Output /\n",
    "                    output_shape = (self.latent_k, ))([mu, sigma])       # Latent Space Representation\n",
    "        zc = concatenate([z, y], axis = 1)                               # Full Decoder Input / Latent Space Representation\n",
    "        if self.display: print(f\"    Encoder Output: {z.shape}\"); print(f\"    Latent Space Representation: {zc.shape}\")\n",
    "\n",
    "        # Decoder Architecture\n",
    "        decoder1 = Dense(   self.latent_k + self.data.params.shape[1],  #\n",
    "                            activation = self.activ)                    #\n",
    "        decoder2 = Dense(   self.pre_shape,                             #\n",
    "                            activation = 'sigmoid')                     #\n",
    "        output = decoder2(decoder1(zc))                                 #\n",
    "\n",
    "        # Decoder Architecture & Pipeline\n",
    "        decoder_input = Input(shape = (self.latent_k + self.data.params.shape[1], ))        #\n",
    "        decoder_output = decoder2(decoder1(decoder_input))                                  #\n",
    "        if self.display: print(f\"    Decoder Input: {decoder_input.shape}\"); print(f\"    Decoder Output: {decoder_output.shape}\")\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # Loss Functions\n",
    "        def vae_loss(y, p):\n",
    "            recon = K.sum(K.binary_crossentropy(y, p), axis = -1)\n",
    "            kl = 0.5 * K.sum(K.exp(sigma) + K.square(mu) - 1.0 - sigma, axis=-1)\n",
    "            return recon + kl\n",
    "\n",
    "        def KL_loss(y, p):\n",
    "            return(0.5 * K.sum(K.exp(sigma) + K.square(mu) - 1.0 - sigma, axis = 1))\n",
    "\n",
    "        def recon_loss(y, p):\n",
    "            return K.sum(K.binary_crossentropy(y, p), axis = -1)\n",
    "        \n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # Full Model Pipeline\n",
    "        self.encoder = Model([X, y], mu)                                # Encoder Model Compilation\n",
    "        self.decoder = Model(decoder_input, decoder_output)             # Decoder Model Compilation\n",
    "        self.model = Model([X, y], output)                              # CVAE Model Compilation\n",
    "        self.model.compile( optimizer = optim,                          # CVAE Optimization Method & Metrics Definition\n",
    "                            loss = vae_loss,\n",
    "                            metrics = [KL_loss, recon_loss])\n",
    "\n",
    "\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    # Model Training Mode\n",
    "    def train(\n",
    "        self,\n",
    "        dataset: str = 'train',                     # Dataset to Execute Training on (Training / Test Set)\n",
    "        n_epochs: int = 500,                        # Number of Epochs for Training Mode of both Datasets\n",
    "        batch_size: int = 500,                      # Batch Size for Training Mode of both Datasets\n",
    "    ):\n",
    "\n",
    "        # Dataset Choice Assertion\n",
    "        self.train_epochs = n_epochs; self.batch_size = batch_size\n",
    "        assert(dataset == 'train' or dataset == 'test'), \"Dataset Chosen not Found!\"\n",
    "        if dataset == 'train':\n",
    "            ds = self.data.train_set\n",
    "            n_patients = self.data.train_patients\n",
    "        else:\n",
    "            ds = self.data.test_set\n",
    "            n_patients = self.data.test_patients\n",
    "\n",
    "        # Keras Training Functionality\n",
    "        self.model_hist = self.model.fit(   [np.array(ds['X_train']), np.array(ds['y_train'])], np.array(ds['X_train']),                # Training Data\n",
    "                                            batch_size = self.batch_size,                                                               # Batch Size\n",
    "                                            epochs = self.train_epochs,                                                                 # Number of Epochs\n",
    "                                            validation_data = [np.array(ds['X_val']), np.array(ds['y_val'])], np.array(ds['X_val']),    # Validation Data\n",
    "                                            callbacks = [EarlyStopping(patience = 5)])                                                  # Inclusion of Early Stopping\n",
    "        #if (~self.display): print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building CVAE Model...\n",
      "    Encoder Input: (?, 1207) [X: (?, 1200) + y: (?, 7)]\n",
      "    Encoder Output: (?, 50)\n",
      "    Latent Space Representation: (?, 57)\n",
      "    Decoder Input: (?, 57)\n",
      "    Decoder Output: (?, 1200)\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 2000 samples, validate on 3376 samples\n",
      "Epoch 1/500\n",
      "2000/2000 [==============================] - 0s 168us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 2/500\n",
      "2000/2000 [==============================] - 0s 67us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 3/500\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 4/500\n",
      "2000/2000 [==============================] - 0s 68us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 5/500\n",
      "2000/2000 [==============================] - 0s 67us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Train on 2000 samples, validate on 3376 samples\n",
      "Epoch 1/500\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 2/500\n",
      "2000/2000 [==============================] - 0s 73us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 3/500\n",
      "2000/2000 [==============================] - 0s 70us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 4/500\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 5/500\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Train on 2000 samples, validate on 3376 samples\n",
      "Epoch 1/500\n",
      "2000/2000 [==============================] - 0s 77us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 2/500\n",
      "2000/2000 [==============================] - 0s 65us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 3/500\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 4/500\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 5/500\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Train on 2000 samples, validate on 3376 samples\n",
      "Epoch 1/500\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 2/500\n",
      "2000/2000 [==============================] - 0s 63us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 3/500\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 4/500\n",
      "2000/2000 [==============================] - 0s 99us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n",
      "Epoch 5/500\n",
      "2000/2000 [==============================] - 0s 85us/sample - loss: nan - KL_loss: nan - recon_loss: nan - val_loss: nan - val_KL_loss: nan - val_recon_loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Model Creation & Training\n",
    "cvae = Keras_CVAE(data = mudi)\n",
    "cvae.train('train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7e2413ca9464f5b18ee008ec75e3890212b75ca17b4a3699f34f03bf3acaeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
